{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-3452d621ff45>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/arturo/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/arturo/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/arturo/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/arturo/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/arturo/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Size of:\n",
      "- Training-set:\t\t55000\n",
      "- Test-set:\t\t10000\n",
      "- Validation-set:\t5000\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(mnist.train.labels)))\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(mnist.test.labels)))\n",
    "print(\"- Validation-set:\\t{}\".format(len(mnist.validation.labels)))\n",
    "\n",
    "# hyper-parameters\n",
    "logs_path = \"./logs/jupyter/embedding/\"  # path to the folder that we want to save the logs for Tensorboard\n",
    "learning_rate = 0.005  # The optimization learning rate\n",
    "epochs = 5  # Total number of training epochs\n",
    "batch_size = 100  # Training batch size\n",
    "display_freq = 100  # Frequency of displaying the training results\n",
    "\n",
    "# Network Parameters\n",
    "# We know that MNIST images are 28 pixels in each dimension.\n",
    "img_h = img_w = 28\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = img_h * img_w\n",
    "\n",
    "# Number of classes, one class for each of 10 digits.\n",
    "n_classes = 10\n",
    "\n",
    "# number of units in the first hidden layer\n",
    "h1 = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight and bais wrappers\n",
    "def weight_variable(name, shape):\n",
    "    \"\"\"\n",
    "    Create a weight variable with appropriate initialization\n",
    "    :param name: weight name\n",
    "    :param shape: weight shape\n",
    "    :return: initialized weight variable\n",
    "    \"\"\"\n",
    "    initer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "    return tf.get_variable('W_' + name,\n",
    "                           dtype=tf.float32,\n",
    "                           shape=shape,\n",
    "                           initializer=initer)\n",
    "\n",
    "def bias_variable(name, shape):\n",
    "    \"\"\"\n",
    "    Create a bias variable with appropriate initialization\n",
    "    :param name: bias variable name\n",
    "    :param shape: bias variable shape\n",
    "    :return: initialized bias variable\n",
    "    \"\"\"\n",
    "    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n",
    "    return tf.get_variable('b_' + name,\n",
    "                           dtype=tf.float32,\n",
    "                           initializer=initial)\n",
    "\n",
    "def fc_layer(x, num_units, name, use_relu=True):\n",
    "    \"\"\"\n",
    "    Create a fully-connected layer\n",
    "    :param x: input from previous layer\n",
    "    :param num_units: number of hidden units in the fully-connected layer\n",
    "    :param name: layer name\n",
    "    :param use_relu: boolean to add ReLU non-linearity (or not)\n",
    "    :return: The output array\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        in_dim = x.get_shape()[1]\n",
    "        W = weight_variable(name, shape=[in_dim, num_units])\n",
    "        tf.summary.histogram('W', W)\n",
    "        b = bias_variable(name, [num_units])\n",
    "        tf.summary.histogram('b', b)\n",
    "        layer = tf.matmul(x, W)\n",
    "        layer += b\n",
    "        if use_relu:\n",
    "            layer = tf.nn.relu(layer)\n",
    "        return layer\n",
    "\n",
    "\n",
    "\n",
    "def write_sprite_image(filename, images):\n",
    "    \"\"\"\n",
    "        Create a sprite image consisting of sample images\n",
    "        :param filename: name of the file to save on disk\n",
    "        :param shape: tensor of flattened images\n",
    "    \"\"\"\n",
    "\n",
    "    # Invert grayscale image\n",
    "    images = 1 - images\n",
    "\n",
    "    # Calculate number of plot\n",
    "    n_plots = int(np.ceil(np.sqrt(images.shape[0])))\n",
    "\n",
    "    # Make the background of sprite image\n",
    "    sprite_image = np.ones((img_h * n_plots, img_w * n_plots))\n",
    "\n",
    "    for i in range(n_plots):\n",
    "        for j in range(n_plots):\n",
    "            img_idx = i * n_plots + j\n",
    "            if img_idx < images.shape[0]:\n",
    "                img = images[img_idx]\n",
    "                sprite_image[i * img_h:(i + 1) * img_h,\n",
    "                j * img_w:(j + 1) * img_w] = img\n",
    "\n",
    "    plt.imsave(filename, sprite_image, cmap='gray')\n",
    "    print('Sprite image saved in {}'.format(filename))\n",
    "\n",
    "def write_metadata(filename, labels):\n",
    "    \"\"\"\n",
    "            Create a metadata file image consisting of sample indices and labels\n",
    "            :param filename: name of the file to save on disk\n",
    "            :param shape: tensor of labels\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"Index\\tLabel\\n\")\n",
    "        for index, label in enumerate(labels):\n",
    "            f.write(\"{}\\t{}\\n\".format(index, label))\n",
    "\n",
    "    print('Metadata file saved in {}'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data set\n",
    "X_test = mnist.test.images\n",
    "Y_test = mnist.test.labels\n",
    "\n",
    "X_train = mnist.train.images\n",
    "Y_train = mnist.train.labels\n",
    "\n",
    "X_val = mnist.validation.images\n",
    "Y_val = mnist.validation.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "g_1=tf.Graph()\n",
    "with g_1.as_default():\n",
    "    # Create graph\n",
    "    # Placeholders for inputs (x), outputs(y)\n",
    "    with tf.variable_scope('Input'):\n",
    "        x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='X')\n",
    "        tf.summary.image('input_image', tf.reshape(x, (-1, img_w, img_h, 1)), max_outputs=5)\n",
    "        y = tf.placeholder(tf.float32, shape=[None, n_classes], name='Y')\n",
    "    #fc1 = fc_layer(x, h1, 'Hidden_layer', use_relu=True)\n",
    "    output_logits = fc_layer(x, n_classes, 'Output_layer', use_relu=False)\n",
    "\n",
    "    # Define the loss function, optimizer, and accuracy\n",
    "    with tf.variable_scope('Train'):\n",
    "        with tf.variable_scope('Loss'):\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name='loss')\n",
    "            tf.summary.scalar('loss', loss)\n",
    "        with tf.variable_scope('Optimizer'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam-op').minimize(loss)\n",
    "        with tf.variable_scope('Accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name='correct_pred')\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "            # Network predictions\n",
    "            cls_prediction = tf.argmax(output_logits, axis=1, name='predictions')\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # Initialize the embedding variable with the shape of our desired tensor\n",
    "    tensor_shape = (X_test.shape[0] , output_logits.get_shape()[1].value) # [test_set , h1] = [10000 , 200]\n",
    "    embedding_var = tf.Variable(tf.zeros(tensor_shape), name='fc1_embedding')\n",
    "    # assign the tensor that we want to visualize to the embedding variable\n",
    "    embedding_assign = embedding_var.assign(output_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "iter   0:\t Loss=2.02,\tTraining Accuracy=55.0%\n",
      "iter 100:\t Loss=0.33,\tTraining Accuracy=91.0%\n",
      "iter 200:\t Loss=0.34,\tTraining Accuracy=90.0%\n",
      "iter 300:\t Loss=0.24,\tTraining Accuracy=95.0%\n",
      "iter 400:\t Loss=0.24,\tTraining Accuracy=97.0%\n",
      "iter 500:\t Loss=0.29,\tTraining Accuracy=92.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 1, validation loss: 0.29, validation accuracy: 92.3%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 2\n",
      "iter   0:\t Loss=0.25,\tTraining Accuracy=93.0%\n",
      "iter 100:\t Loss=0.23,\tTraining Accuracy=96.0%\n",
      "iter 200:\t Loss=0.30,\tTraining Accuracy=92.0%\n",
      "iter 300:\t Loss=0.34,\tTraining Accuracy=93.0%\n",
      "iter 400:\t Loss=0.43,\tTraining Accuracy=86.0%\n",
      "iter 500:\t Loss=0.25,\tTraining Accuracy=94.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 2, validation loss: 0.27, validation accuracy: 92.3%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 3\n",
      "iter   0:\t Loss=0.27,\tTraining Accuracy=93.0%\n",
      "iter 100:\t Loss=0.38,\tTraining Accuracy=90.0%\n",
      "iter 200:\t Loss=0.12,\tTraining Accuracy=98.0%\n",
      "iter 300:\t Loss=0.31,\tTraining Accuracy=90.0%\n",
      "iter 400:\t Loss=0.32,\tTraining Accuracy=92.0%\n",
      "iter 500:\t Loss=0.21,\tTraining Accuracy=96.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 3, validation loss: 0.26, validation accuracy: 92.8%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 4\n",
      "iter   0:\t Loss=0.42,\tTraining Accuracy=91.0%\n",
      "iter 100:\t Loss=0.16,\tTraining Accuracy=93.0%\n",
      "iter 200:\t Loss=0.29,\tTraining Accuracy=92.0%\n",
      "iter 300:\t Loss=0.23,\tTraining Accuracy=90.0%\n",
      "iter 400:\t Loss=0.27,\tTraining Accuracy=92.0%\n",
      "iter 500:\t Loss=0.32,\tTraining Accuracy=91.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 4, validation loss: 0.27, validation accuracy: 92.9%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 5\n",
      "iter   0:\t Loss=0.26,\tTraining Accuracy=94.0%\n",
      "iter 100:\t Loss=0.33,\tTraining Accuracy=94.0%\n",
      "iter 200:\t Loss=0.51,\tTraining Accuracy=88.0%\n",
      "iter 300:\t Loss=0.16,\tTraining Accuracy=94.0%\n",
      "iter 400:\t Loss=0.31,\tTraining Accuracy=98.0%\n",
      "iter 500:\t Loss=0.43,\tTraining Accuracy=89.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 5, validation loss: 0.27, validation accuracy: 92.5%\n",
      "---------------------------------------------------------\n",
      "Sprite image saved in ./logs/jupyter/embedding/sprite_images.png\n",
      "Metadata file saved in ./logs/jupyter/embedding/metadata.tsv\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph (session)\n",
    "#sess = tf.InteractiveSession() # using InteractiveSession instead of Session to test network in separate cell\n",
    "with tf.Session(graph=g_1) as sess:\n",
    "    sess.run(init)\n",
    "    train_writer = tf.summary.FileWriter(logs_path, sess.graph)\n",
    "    num_tr_iter = int(mnist.train.num_examples / batch_size)\n",
    "    global_step = 0\n",
    "    for epoch in range(epochs):\n",
    "        print('Training epoch: {}'.format(epoch + 1))\n",
    "        for iteration in range(num_tr_iter):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            global_step += 1\n",
    "            # Run optimization op (backprop)\n",
    "            feed_dict_batch = {x: batch_x, y: batch_y}\n",
    "            _, summary_tr = sess.run([optimizer, merged], feed_dict=feed_dict_batch)\n",
    "            train_writer.add_summary(summary_tr, global_step)\n",
    "\n",
    "            if iteration % display_freq == 0:\n",
    "                # Calculate and display the batch loss and accuracy\n",
    "                loss_batch, acc_batch = sess.run([loss, accuracy],feed_dict=feed_dict_batch)\n",
    "                print(\"iter {0:3d}:\\t Loss={1:.2f},\\tTraining Accuracy={2:.01%}\".format(iteration, loss_batch, acc_batch))\n",
    "\n",
    "        # Run validation after every epoch\n",
    "        feed_dict_valid = {x: mnist.validation.images, y: mnist.validation.labels}\n",
    "        loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n",
    "        print('---------------------------------------------------------')\n",
    "        print(\"Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}\".format(epoch + 1, loss_valid, acc_valid))\n",
    "        print('---------------------------------------------------------')\n",
    "\n",
    "    from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "    # Create a config object to write the configuration parameters\n",
    "    config = projector.ProjectorConfig()\n",
    "\n",
    "    # Add embedding variable\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = embedding_var.name\n",
    "\n",
    "    # Link this tensor to its metadata file (e.g. labels) -> we will create this file later\n",
    "    embedding.metadata_path = 'metadata.tsv'\n",
    "\n",
    "    # Specify where you find the sprite. -> we will create this image later\n",
    "    embedding.sprite.image_path = 'sprite_images.png'\n",
    "    embedding.sprite.single_image_dim.extend([img_w, img_h])\n",
    "\n",
    "    # Write a projector_config.pbtxt in the logs_path.\n",
    "    # TensorBoard will read this file during startup.\n",
    "    projector.visualize_embeddings(train_writer, config)\n",
    "\n",
    "    # Run session to evaluate the tensor\n",
    "    x_test_fc1 = sess.run(embedding_assign, feed_dict={x: X_test})\n",
    "\n",
    "    # Save the tensor in model.ckpt file\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, os.path.join(logs_path, \"model.ckpt\"), global_step)\n",
    "\n",
    "\n",
    "    # Reshape images from vector to matrix\n",
    "    x_test_images = np.reshape(np.array(X_test), (-1, img_w, img_h))\n",
    "    # Reshape labels from one-hot-encode to index\n",
    "    x_test_labels = np.argmax(Y_test, axis=1)\n",
    "\n",
    "    write_sprite_image(os.path.join(logs_path, 'sprite_images.png'), x_test_images)\n",
    "    write_metadata(os.path.join(logs_path, 'metadata.tsv'), x_test_labels)\n",
    "\n",
    "    # close the session after you are done with testing\n",
    "    #sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 1\n",
      "Finished Epoch 2\n",
      "Finished Epoch 3\n",
      "Finished Epoch 4\n",
      "Finished Epoch 5\n",
      "Finished Epoch 6\n",
      "Finished Epoch 7\n",
      "Finished Epoch 8\n",
      "Finished Epoch 9\n",
      "Finished Epoch 10\n",
      "Finished Epoch 11\n",
      "Finished Epoch 12\n",
      "Finished Epoch 13\n",
      "Finished Epoch 14\n",
      "Finished Epoch 15\n",
      "Finished Epoch 16\n",
      "Finished Epoch 17\n",
      "Finished Epoch 18\n",
      "Finished Epoch 19\n",
      "Finished Epoch 20\n",
      "Sprite image saved in logs/myjupyter16:14:31/sprite_images.png\n",
      "Metadata file saved in logs/myjupyter16:14:31/metadata.tsv\n"
     ]
    }
   ],
   "source": [
    "g_train=tf.Graph()\n",
    "with g_train.as_default():\n",
    "\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        X = tf.placeholder(tf.float32, [None, 784],name='x_data')\n",
    "        tf.summary.image('input_image', tf.reshape(X, (-1, 28, 28, 1)), max_outputs=5)\n",
    "        Ydata = tf.placeholder(tf.float32, [None, 10],name='y_data') \n",
    "    \n",
    "    with tf.name_scope(\"fitting_variables\"):\n",
    "        with tf.name_scope(\"weights\"):\n",
    "            #initer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "            W = tf.Variable(tf.zeros([784,10]),name='w')\n",
    "            tf.summary.histogram(\"wieghts\",W)\n",
    "        with tf.name_scope(\"bias\"):\n",
    "            b = tf.Variable(tf.ones([10]),name='b')\n",
    "            tf.summary.histogram(\"biases\",b)\n",
    "    \n",
    "    with tf.name_scope(\"operations\"):\n",
    "        prod=tf.matmul(X,W,name=\"mult\") #transponemos W^t X^t ==> X W. \n",
    "        # cada dato es una fila en X, la matriz W la renombramos como W^t para tener W al final\n",
    "        sumation=prod+b #sumar b verticalmente con broadcasting\n",
    "        \n",
    "    with tf.name_scope(\"outputs\"):\n",
    "        Ymod = tf.nn.softmax(sumation,name=\"Ymod\") \n",
    "    \n",
    "    with tf.name_scope(\"evaluation\"):\n",
    "        with tf.name_scope(\"cross_entropy\"):\n",
    "            cross_entropy = tf.reduce_mean(-tf.reduce_sum(Ydata * tf.log(Ymod+1e-8), reduction_indices=[1])) \n",
    "            tf.summary.scalar(\"cross_entropy\",cross_entropy)\n",
    "        \n",
    "        with tf.name_scope(\"correct_prediction\"):\n",
    "            correct_prediction = tf.equal(tf.argmax(Ymod,axis=1), tf.argmax(Ydata,axis=1)) \n",
    "            #tf.summary.tensor_summary(\"correct_prediction\",correct_prediction)\n",
    "        \n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n",
    "            tf.summary.scalar(\"accuracy\",accuracy)\n",
    "        \n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate=0.01, name='Adam-op').minimize(cross_entropy)\n",
    "        \n",
    "    init=tf.global_variables_initializer()\n",
    "    merged=tf.summary.merge_all()\n",
    "    \n",
    "    # Initialize the embedding variable with the shape of our desired tensor\n",
    "    tensor_shape = (X_test.shape[0] , Ymod.get_shape()[1].value) # [test_set , h1] = [10000 , 200]\n",
    "    embedding_var = tf.Variable(tf.zeros(tensor_shape), name='ymod_embedding')\n",
    "    # assign the tensor that we want to visualize to the embedding variable\n",
    "    embedding_assign = embedding_var.assign(Ymod)\n",
    "        \n",
    "import time\n",
    "\n",
    "batch_size=100\n",
    "no_of_epochs=20\n",
    "\n",
    "iteration_loss=[]\n",
    "iteration_acc=[]\n",
    "\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(graph=g_train) as sess1:\n",
    "    \n",
    "    summaries_dir=\"logs/myjupyter\"\n",
    "    time_stamp=time.strftime(\"%H:%M:%S\") #string con formato hora:minuto:segundo\n",
    "    summaries_dir=summaries_dir+time_stamp\n",
    "    \n",
    "    #directorios para guardar logs de summary, se utilizan en tensorboard\n",
    "    train_writer = tf.summary.FileWriter(summaries_dir+'/train',sess1.graph)\n",
    "    valid_writer = tf.summary.FileWriter(summaries_dir + '/valid',sess1.graph)\n",
    "    test_writer = tf.summary.FileWriter(summaries_dir + '/test',sess1.graph)\n",
    "\n",
    "    init.run()\n",
    "    \n",
    "    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    run_metadata = tf.RunMetadata()\n",
    "    \n",
    "    #calculo inicial de accuracy, perdida,etc.\n",
    "    summary,loss=sess1.run([merged,cross_entropy],feed_dict={X: X_train,Ydata: Y_train},\n",
    "                            options=run_options,run_metadata=run_metadata) #adds metadada to summary \n",
    "    train_writer.add_summary(summary, 0)\n",
    "    train_writer.add_run_metadata(run_metadata, 'loss_epoch%d' % 0)\n",
    "    #print(\"Epoch:=\",0,\"; \\t Epoch Loss:=\",loss)\n",
    "    \n",
    "    summary,t_acc=sess1.run([merged,accuracy], feed_dict={X: X_train, Ydata: Y_train},\n",
    "                             options=run_options,run_metadata=run_metadata) \n",
    "    train_writer.add_summary(summary, 0)\n",
    "    train_writer.add_run_metadata(run_metadata, 'train_epoch%d' % 0)\n",
    "    #print(\"Training Accuracy is\", t_acc*100,\"%\")\n",
    "    \n",
    "    summary,v_acc=sess1.run([merged,accuracy], feed_dict={X: X_val,Ydata:Y_val},\n",
    "                             options=run_options,run_metadata=run_metadata) #adds metadada to summary \n",
    "    valid_writer.add_summary(summary,0)\n",
    "    valid_writer.add_run_metadata(run_metadata, 'valid_epoch%d' % 0)\n",
    "    #print(\"Validation Accuracy is\", v_acc*100,\"%\")\n",
    "    \n",
    "    summary,f_acc=sess1.run([merged,accuracy], feed_dict={X: X_test,Ydata:Y_test},\n",
    "                             options=run_options,run_metadata=run_metadata) #adds metadada to summary \n",
    "    test_writer.add_summary(summary,0)\n",
    "    test_writer.add_run_metadata(run_metadata, 'test_epoch%d' % 0)\n",
    "    #print(\"Test Accuracy is\", f_acc*100,\"%\")\n",
    "    \n",
    "    sum_data=np.array([0,loss,t_acc,v_acc,f_acc]) #agregar los datos de la epoca 0 en una lista\n",
    "    \n",
    "    \n",
    "    \n",
    "    for epoch in range(no_of_epochs):\n",
    "        epoch_loss=0\n",
    "        \n",
    "        for i in range(int(mnist.train.num_examples/batch_size)): #una epoca se divide según el tamaño del batch \n",
    "            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            run_metadata = tf.RunMetadata()\n",
    "            \n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size) \n",
    "            train,loss,acc=sess1.run([train_step,cross_entropy,accuracy], feed_dict={X: batch_xs, Ydata: batch_ys},\n",
    "                                     options=run_options,run_metadata=run_metadata)\n",
    "            \n",
    "            if i==0:\n",
    "                train_writer.add_run_metadata(run_metadata, 'train_step%d' % (epoch))\n",
    "            #calcular gradiente y pérdida sobre los datos del batch (se usan todas la variables de la gráfica)\n",
    "            \n",
    "            epoch_loss+=loss #calcula la pérdida por cada batch y la suma para tener la pérdida final en la época\n",
    "            iteration_loss.append(loss)\n",
    "            iteration_acc.append(acc)\n",
    "        \n",
    "        #print(\"Epoch:=\",(epoch+1),\"; \\t Epoch Loss:=\",epoch_loss)\n",
    "        \n",
    "        \n",
    "        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "        run_metadata = tf.RunMetadata()\n",
    "        \n",
    "        summary,t_acc=sess1.run([merged,accuracy], feed_dict={X: X_train, Ydata: Y_train}, #calculates training accuracy \n",
    "                               options=run_options,run_metadata=run_metadata) #adds metadada to summary       \n",
    "        train_writer.add_summary(summary, (epoch+1))\n",
    "        train_writer.add_run_metadata(run_metadata, 'train_epoch%d' % (epoch+1))\n",
    "        #print(\"Training Accuracy is\", t_acc*100,\"%\")\n",
    "        \n",
    "        summary,v_acc=sess1.run([merged,accuracy], feed_dict={X: X_val,Ydata:Y_val},\n",
    "                               options=run_options,run_metadata=run_metadata)\n",
    "        valid_writer.add_summary(summary, (epoch+1))\n",
    "        valid_writer.add_run_metadata(run_metadata, 'valid_epoch%d' % (epoch+1))\n",
    "        #print(\"Validation Accuracy is\", v_acc*100,\"%\")\n",
    "        \n",
    "        summary,f_acc=sess1.run([merged,accuracy], feed_dict={X: X_test,Ydata:Y_test},\n",
    "                                options=run_options,run_metadata=run_metadata)\n",
    "        test_writer.add_summary(summary, (epoch+1))\n",
    "        test_writer.add_run_metadata(run_metadata, 'test_epoch%d' % (epoch+1))\n",
    "        #print(\"Test Accuracy is\", f_acc*100,\"%\")\n",
    "        \n",
    "        sum_data=np.vstack((sum_data,np.array([(epoch+1),epoch_loss,t_acc,v_acc,f_acc])))\n",
    "        print(\"Finished Epoch\",(epoch+1))\n",
    "        \n",
    "    from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "    # Create a config object to write the configuration parameters\n",
    "    config = projector.ProjectorConfig()\n",
    "\n",
    "    # Add embedding variable\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = embedding_var.name\n",
    "\n",
    "    # Link this tensor to its metadata file (e.g. labels) -> we will create this file later\n",
    "    embedding.metadata_path = 'metadata.tsv'\n",
    "\n",
    "    # Specify where you find the sprite. -> we will create this image later\n",
    "    embedding.sprite.image_path = 'sprite_images.png'\n",
    "    embedding.sprite.single_image_dim.extend([img_w, img_h])\n",
    "\n",
    "    # Write a projector_config.pbtxt in the logs_path.\n",
    "    # TensorBoard will read this file during startup.\n",
    "    embedding_writer = tf.summary.FileWriter(summaries_dir,sess1.graph)\n",
    "    projector.visualize_embeddings(embedding_writer, config)\n",
    "\n",
    "    # Run session to evaluate the tensor\n",
    "    x_test_fc1 = sess1.run(embedding_assign, feed_dict={X: X_test})\n",
    "\n",
    "    # Save the tensor in model.ckpt file\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess1, os.path.join(summaries_dir, \"model.ckpt\"), global_step)\n",
    "\n",
    "\n",
    "    # Reshape images from vector to matrix\n",
    "    x_test_images = np.reshape(np.array(X_test), (-1, img_w, img_h))\n",
    "    # Reshape labels from one-hot-encode to index\n",
    "    x_test_labels = np.argmax(Y_test, axis=1)\n",
    "\n",
    "    write_sprite_image(os.path.join(summaries_dir, 'sprite_images.png'), x_test_images)\n",
    "    write_metadata(os.path.join(summaries_dir, 'metadata.tsv'), x_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
