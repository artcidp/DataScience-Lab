{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema 1: Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lo largo del curso hemos visto diferentes técnicas para entrenar algoritmos. En este problema buscamos implenentar las técnicas que conocemos para resolver el problema de clasificación en la base de datos de fraudes de tarjetas de crédito."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos por importar los datos y las librerías relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>40.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>93.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.449044</td>\n",
       "      <td>-1.176339</td>\n",
       "      <td>0.913860</td>\n",
       "      <td>-1.375667</td>\n",
       "      <td>-1.971383</td>\n",
       "      <td>-0.629152</td>\n",
       "      <td>-1.423236</td>\n",
       "      <td>0.048456</td>\n",
       "      <td>-1.720408</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009302</td>\n",
       "      <td>0.313894</td>\n",
       "      <td>0.027740</td>\n",
       "      <td>0.500512</td>\n",
       "      <td>0.251367</td>\n",
       "      <td>-0.129478</td>\n",
       "      <td>0.042850</td>\n",
       "      <td>0.016253</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.384978</td>\n",
       "      <td>0.616109</td>\n",
       "      <td>-0.874300</td>\n",
       "      <td>-0.094019</td>\n",
       "      <td>2.924584</td>\n",
       "      <td>3.317027</td>\n",
       "      <td>0.470455</td>\n",
       "      <td>0.538247</td>\n",
       "      <td>-0.558895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049924</td>\n",
       "      <td>0.238422</td>\n",
       "      <td>0.009130</td>\n",
       "      <td>0.996710</td>\n",
       "      <td>-0.767315</td>\n",
       "      <td>-0.492208</td>\n",
       "      <td>0.042472</td>\n",
       "      <td>-0.054337</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.249999</td>\n",
       "      <td>-1.221637</td>\n",
       "      <td>0.383930</td>\n",
       "      <td>-1.234899</td>\n",
       "      <td>-1.485419</td>\n",
       "      <td>-0.753230</td>\n",
       "      <td>-0.689405</td>\n",
       "      <td>-0.227487</td>\n",
       "      <td>-2.094011</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231809</td>\n",
       "      <td>-0.483285</td>\n",
       "      <td>0.084668</td>\n",
       "      <td>0.392831</td>\n",
       "      <td>0.161135</td>\n",
       "      <td>-0.354990</td>\n",
       "      <td>0.026416</td>\n",
       "      <td>0.042422</td>\n",
       "      <td>121.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1.069374</td>\n",
       "      <td>0.287722</td>\n",
       "      <td>0.828613</td>\n",
       "      <td>2.712520</td>\n",
       "      <td>-0.178398</td>\n",
       "      <td>0.337544</td>\n",
       "      <td>-0.096717</td>\n",
       "      <td>0.115982</td>\n",
       "      <td>-0.221083</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036876</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.071407</td>\n",
       "      <td>0.104744</td>\n",
       "      <td>0.548265</td>\n",
       "      <td>0.104094</td>\n",
       "      <td>0.021491</td>\n",
       "      <td>0.021293</td>\n",
       "      <td>27.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-2.791855</td>\n",
       "      <td>-0.327771</td>\n",
       "      <td>1.641750</td>\n",
       "      <td>1.767473</td>\n",
       "      <td>-0.136588</td>\n",
       "      <td>0.807596</td>\n",
       "      <td>-0.422911</td>\n",
       "      <td>-1.907107</td>\n",
       "      <td>0.755713</td>\n",
       "      <td>...</td>\n",
       "      <td>1.151663</td>\n",
       "      <td>0.222182</td>\n",
       "      <td>1.020586</td>\n",
       "      <td>0.028317</td>\n",
       "      <td>-0.232746</td>\n",
       "      <td>-0.235557</td>\n",
       "      <td>-0.164778</td>\n",
       "      <td>-0.030154</td>\n",
       "      <td>58.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.752417</td>\n",
       "      <td>0.345485</td>\n",
       "      <td>2.057323</td>\n",
       "      <td>-1.468643</td>\n",
       "      <td>-1.158394</td>\n",
       "      <td>-0.077850</td>\n",
       "      <td>-0.608581</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>-0.436167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499625</td>\n",
       "      <td>1.353650</td>\n",
       "      <td>-0.256573</td>\n",
       "      <td>-0.065084</td>\n",
       "      <td>-0.039124</td>\n",
       "      <td>-0.087086</td>\n",
       "      <td>-0.180998</td>\n",
       "      <td>0.129394</td>\n",
       "      <td>15.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1.103215</td>\n",
       "      <td>-0.040296</td>\n",
       "      <td>1.267332</td>\n",
       "      <td>1.289091</td>\n",
       "      <td>-0.735997</td>\n",
       "      <td>0.288069</td>\n",
       "      <td>-0.586057</td>\n",
       "      <td>0.189380</td>\n",
       "      <td>0.782333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024612</td>\n",
       "      <td>0.196002</td>\n",
       "      <td>0.013802</td>\n",
       "      <td>0.103758</td>\n",
       "      <td>0.364298</td>\n",
       "      <td>-0.382261</td>\n",
       "      <td>0.092809</td>\n",
       "      <td>0.037051</td>\n",
       "      <td>12.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>13.0</td>\n",
       "      <td>-0.436905</td>\n",
       "      <td>0.918966</td>\n",
       "      <td>0.924591</td>\n",
       "      <td>-0.727219</td>\n",
       "      <td>0.915679</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>0.707642</td>\n",
       "      <td>0.087962</td>\n",
       "      <td>-0.665271</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.194796</td>\n",
       "      <td>-0.672638</td>\n",
       "      <td>-0.156858</td>\n",
       "      <td>-0.888386</td>\n",
       "      <td>-0.342413</td>\n",
       "      <td>-0.049027</td>\n",
       "      <td>0.079692</td>\n",
       "      <td>0.131024</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.0</td>\n",
       "      <td>-5.401258</td>\n",
       "      <td>-5.450148</td>\n",
       "      <td>1.186305</td>\n",
       "      <td>1.736239</td>\n",
       "      <td>3.049106</td>\n",
       "      <td>-1.763406</td>\n",
       "      <td>-1.559738</td>\n",
       "      <td>0.160842</td>\n",
       "      <td>1.233090</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.503600</td>\n",
       "      <td>0.984460</td>\n",
       "      <td>2.458589</td>\n",
       "      <td>0.042119</td>\n",
       "      <td>-0.481631</td>\n",
       "      <td>-0.621272</td>\n",
       "      <td>0.392053</td>\n",
       "      <td>0.949594</td>\n",
       "      <td>46.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.492936</td>\n",
       "      <td>-1.029346</td>\n",
       "      <td>0.454795</td>\n",
       "      <td>-1.438026</td>\n",
       "      <td>-1.555434</td>\n",
       "      <td>-0.720961</td>\n",
       "      <td>-1.080664</td>\n",
       "      <td>-0.053127</td>\n",
       "      <td>-1.978682</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177650</td>\n",
       "      <td>-0.175074</td>\n",
       "      <td>0.040002</td>\n",
       "      <td>0.295814</td>\n",
       "      <td>0.332931</td>\n",
       "      <td>-0.220385</td>\n",
       "      <td>0.022298</td>\n",
       "      <td>0.007602</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0.694885</td>\n",
       "      <td>-1.361819</td>\n",
       "      <td>1.029221</td>\n",
       "      <td>0.834159</td>\n",
       "      <td>-1.191209</td>\n",
       "      <td>1.309109</td>\n",
       "      <td>-0.878586</td>\n",
       "      <td>0.445290</td>\n",
       "      <td>-0.446196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.295583</td>\n",
       "      <td>-0.571955</td>\n",
       "      <td>-0.050881</td>\n",
       "      <td>-0.304215</td>\n",
       "      <td>0.072001</td>\n",
       "      <td>-0.422234</td>\n",
       "      <td>0.086553</td>\n",
       "      <td>0.063499</td>\n",
       "      <td>231.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.962496</td>\n",
       "      <td>0.328461</td>\n",
       "      <td>-0.171479</td>\n",
       "      <td>2.109204</td>\n",
       "      <td>1.129566</td>\n",
       "      <td>1.696038</td>\n",
       "      <td>0.107712</td>\n",
       "      <td>0.521502</td>\n",
       "      <td>-1.191311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143997</td>\n",
       "      <td>0.402492</td>\n",
       "      <td>-0.048508</td>\n",
       "      <td>-1.371866</td>\n",
       "      <td>0.390814</td>\n",
       "      <td>0.199964</td>\n",
       "      <td>0.016371</td>\n",
       "      <td>-0.014605</td>\n",
       "      <td>34.09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>18.0</td>\n",
       "      <td>1.166616</td>\n",
       "      <td>0.502120</td>\n",
       "      <td>-0.067300</td>\n",
       "      <td>2.261569</td>\n",
       "      <td>0.428804</td>\n",
       "      <td>0.089474</td>\n",
       "      <td>0.241147</td>\n",
       "      <td>0.138082</td>\n",
       "      <td>-0.989162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018702</td>\n",
       "      <td>-0.061972</td>\n",
       "      <td>-0.103855</td>\n",
       "      <td>-0.370415</td>\n",
       "      <td>0.603200</td>\n",
       "      <td>0.108556</td>\n",
       "      <td>-0.040521</td>\n",
       "      <td>-0.011418</td>\n",
       "      <td>2.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.247491</td>\n",
       "      <td>0.277666</td>\n",
       "      <td>1.185471</td>\n",
       "      <td>-0.092603</td>\n",
       "      <td>-1.314394</td>\n",
       "      <td>-0.150116</td>\n",
       "      <td>-0.946365</td>\n",
       "      <td>-1.617935</td>\n",
       "      <td>1.544071</td>\n",
       "      <td>...</td>\n",
       "      <td>1.650180</td>\n",
       "      <td>0.200454</td>\n",
       "      <td>-0.185353</td>\n",
       "      <td>0.423073</td>\n",
       "      <td>0.820591</td>\n",
       "      <td>-0.227632</td>\n",
       "      <td>0.336634</td>\n",
       "      <td>0.250475</td>\n",
       "      <td>22.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>22.0</td>\n",
       "      <td>-1.946525</td>\n",
       "      <td>-0.044901</td>\n",
       "      <td>-0.405570</td>\n",
       "      <td>-1.013057</td>\n",
       "      <td>2.941968</td>\n",
       "      <td>2.955053</td>\n",
       "      <td>-0.063063</td>\n",
       "      <td>0.855546</td>\n",
       "      <td>0.049967</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.579526</td>\n",
       "      <td>-0.799229</td>\n",
       "      <td>0.870300</td>\n",
       "      <td>0.983421</td>\n",
       "      <td>0.321201</td>\n",
       "      <td>0.149650</td>\n",
       "      <td>0.707519</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>22.0</td>\n",
       "      <td>-2.074295</td>\n",
       "      <td>-0.121482</td>\n",
       "      <td>1.322021</td>\n",
       "      <td>0.410008</td>\n",
       "      <td>0.295198</td>\n",
       "      <td>-0.959537</td>\n",
       "      <td>0.543985</td>\n",
       "      <td>-0.104627</td>\n",
       "      <td>0.475664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.403639</td>\n",
       "      <td>-0.227404</td>\n",
       "      <td>0.742435</td>\n",
       "      <td>0.398535</td>\n",
       "      <td>0.249212</td>\n",
       "      <td>0.274404</td>\n",
       "      <td>0.359969</td>\n",
       "      <td>0.243232</td>\n",
       "      <td>26.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>23.0</td>\n",
       "      <td>1.173285</td>\n",
       "      <td>0.353498</td>\n",
       "      <td>0.283905</td>\n",
       "      <td>1.133563</td>\n",
       "      <td>-0.172577</td>\n",
       "      <td>-0.916054</td>\n",
       "      <td>0.369025</td>\n",
       "      <td>-0.327260</td>\n",
       "      <td>-0.246651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067003</td>\n",
       "      <td>0.227812</td>\n",
       "      <td>-0.150487</td>\n",
       "      <td>0.435045</td>\n",
       "      <td>0.724825</td>\n",
       "      <td>-0.337082</td>\n",
       "      <td>0.016368</td>\n",
       "      <td>0.030041</td>\n",
       "      <td>41.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>23.0</td>\n",
       "      <td>1.322707</td>\n",
       "      <td>-0.174041</td>\n",
       "      <td>0.434555</td>\n",
       "      <td>0.576038</td>\n",
       "      <td>-0.836758</td>\n",
       "      <td>-0.831083</td>\n",
       "      <td>-0.264905</td>\n",
       "      <td>-0.220982</td>\n",
       "      <td>-1.071425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.284376</td>\n",
       "      <td>-0.323357</td>\n",
       "      <td>-0.037710</td>\n",
       "      <td>0.347151</td>\n",
       "      <td>0.559639</td>\n",
       "      <td>-0.280158</td>\n",
       "      <td>0.042335</td>\n",
       "      <td>0.028822</td>\n",
       "      <td>16.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>23.0</td>\n",
       "      <td>-0.414289</td>\n",
       "      <td>0.905437</td>\n",
       "      <td>1.727453</td>\n",
       "      <td>1.473471</td>\n",
       "      <td>0.007443</td>\n",
       "      <td>-0.200331</td>\n",
       "      <td>0.740228</td>\n",
       "      <td>-0.029247</td>\n",
       "      <td>-0.593392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077237</td>\n",
       "      <td>0.457331</td>\n",
       "      <td>-0.038500</td>\n",
       "      <td>0.642522</td>\n",
       "      <td>-0.183891</td>\n",
       "      <td>-0.277464</td>\n",
       "      <td>0.182687</td>\n",
       "      <td>0.152665</td>\n",
       "      <td>33.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>23.0</td>\n",
       "      <td>1.059387</td>\n",
       "      <td>-0.175319</td>\n",
       "      <td>1.266130</td>\n",
       "      <td>1.186110</td>\n",
       "      <td>-0.786002</td>\n",
       "      <td>0.578435</td>\n",
       "      <td>-0.767084</td>\n",
       "      <td>0.401046</td>\n",
       "      <td>0.699500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013676</td>\n",
       "      <td>0.213734</td>\n",
       "      <td>0.014462</td>\n",
       "      <td>0.002951</td>\n",
       "      <td>0.294638</td>\n",
       "      <td>-0.395070</td>\n",
       "      <td>0.081461</td>\n",
       "      <td>0.024220</td>\n",
       "      <td>12.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284777</th>\n",
       "      <td>172764.0</td>\n",
       "      <td>2.079137</td>\n",
       "      <td>-0.028723</td>\n",
       "      <td>-1.343392</td>\n",
       "      <td>0.358000</td>\n",
       "      <td>-0.045791</td>\n",
       "      <td>-1.345452</td>\n",
       "      <td>0.227476</td>\n",
       "      <td>-0.378355</td>\n",
       "      <td>0.665911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235758</td>\n",
       "      <td>0.829758</td>\n",
       "      <td>-0.002063</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.262183</td>\n",
       "      <td>-0.105327</td>\n",
       "      <td>-0.022363</td>\n",
       "      <td>-0.060283</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284778</th>\n",
       "      <td>172764.0</td>\n",
       "      <td>-0.764523</td>\n",
       "      <td>0.588379</td>\n",
       "      <td>-0.907599</td>\n",
       "      <td>-0.418847</td>\n",
       "      <td>0.901528</td>\n",
       "      <td>-0.760802</td>\n",
       "      <td>0.758545</td>\n",
       "      <td>0.414698</td>\n",
       "      <td>-0.730854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003530</td>\n",
       "      <td>-0.431876</td>\n",
       "      <td>0.141759</td>\n",
       "      <td>0.587119</td>\n",
       "      <td>-0.200998</td>\n",
       "      <td>0.267337</td>\n",
       "      <td>-0.152951</td>\n",
       "      <td>-0.065285</td>\n",
       "      <td>80.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284779</th>\n",
       "      <td>172766.0</td>\n",
       "      <td>1.975178</td>\n",
       "      <td>-0.616244</td>\n",
       "      <td>-2.628295</td>\n",
       "      <td>-0.406246</td>\n",
       "      <td>2.327804</td>\n",
       "      <td>3.664740</td>\n",
       "      <td>-0.533297</td>\n",
       "      <td>0.842937</td>\n",
       "      <td>1.128798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086043</td>\n",
       "      <td>0.543613</td>\n",
       "      <td>-0.032129</td>\n",
       "      <td>0.768379</td>\n",
       "      <td>0.477688</td>\n",
       "      <td>-0.031833</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>-0.066542</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284780</th>\n",
       "      <td>172766.0</td>\n",
       "      <td>-1.727503</td>\n",
       "      <td>1.108356</td>\n",
       "      <td>2.219561</td>\n",
       "      <td>1.148583</td>\n",
       "      <td>-0.884199</td>\n",
       "      <td>0.793083</td>\n",
       "      <td>-0.527298</td>\n",
       "      <td>0.866429</td>\n",
       "      <td>0.853819</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094708</td>\n",
       "      <td>0.236818</td>\n",
       "      <td>-0.204280</td>\n",
       "      <td>1.158185</td>\n",
       "      <td>0.627801</td>\n",
       "      <td>-0.399981</td>\n",
       "      <td>0.510818</td>\n",
       "      <td>0.233265</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284781</th>\n",
       "      <td>172766.0</td>\n",
       "      <td>-1.139015</td>\n",
       "      <td>-0.155510</td>\n",
       "      <td>1.894478</td>\n",
       "      <td>-1.138957</td>\n",
       "      <td>1.451777</td>\n",
       "      <td>0.093598</td>\n",
       "      <td>0.191353</td>\n",
       "      <td>0.092211</td>\n",
       "      <td>-0.062621</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191027</td>\n",
       "      <td>-0.631658</td>\n",
       "      <td>-0.147249</td>\n",
       "      <td>0.212931</td>\n",
       "      <td>0.354257</td>\n",
       "      <td>-0.241068</td>\n",
       "      <td>-0.161717</td>\n",
       "      <td>-0.149188</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284782</th>\n",
       "      <td>172767.0</td>\n",
       "      <td>-0.268061</td>\n",
       "      <td>2.540315</td>\n",
       "      <td>-1.400915</td>\n",
       "      <td>4.846661</td>\n",
       "      <td>0.639105</td>\n",
       "      <td>0.186479</td>\n",
       "      <td>-0.045911</td>\n",
       "      <td>0.936448</td>\n",
       "      <td>-2.419986</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.263889</td>\n",
       "      <td>-0.857904</td>\n",
       "      <td>0.235172</td>\n",
       "      <td>-0.681794</td>\n",
       "      <td>-0.668894</td>\n",
       "      <td>0.044657</td>\n",
       "      <td>-0.066751</td>\n",
       "      <td>-0.072447</td>\n",
       "      <td>12.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284783</th>\n",
       "      <td>172768.0</td>\n",
       "      <td>-1.796092</td>\n",
       "      <td>1.929178</td>\n",
       "      <td>-2.828417</td>\n",
       "      <td>-1.689844</td>\n",
       "      <td>2.199572</td>\n",
       "      <td>3.123732</td>\n",
       "      <td>-0.270714</td>\n",
       "      <td>1.657495</td>\n",
       "      <td>0.465804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271170</td>\n",
       "      <td>1.145750</td>\n",
       "      <td>0.084783</td>\n",
       "      <td>0.721269</td>\n",
       "      <td>-0.529906</td>\n",
       "      <td>-0.240117</td>\n",
       "      <td>0.129126</td>\n",
       "      <td>-0.080620</td>\n",
       "      <td>11.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284784</th>\n",
       "      <td>172768.0</td>\n",
       "      <td>-0.669662</td>\n",
       "      <td>0.923769</td>\n",
       "      <td>-1.543167</td>\n",
       "      <td>-1.560729</td>\n",
       "      <td>2.833960</td>\n",
       "      <td>3.240843</td>\n",
       "      <td>0.181576</td>\n",
       "      <td>1.282746</td>\n",
       "      <td>-0.893890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183856</td>\n",
       "      <td>0.202670</td>\n",
       "      <td>-0.373023</td>\n",
       "      <td>0.651122</td>\n",
       "      <td>1.073823</td>\n",
       "      <td>0.844590</td>\n",
       "      <td>-0.286676</td>\n",
       "      <td>-0.187719</td>\n",
       "      <td>40.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284785</th>\n",
       "      <td>172768.0</td>\n",
       "      <td>0.032887</td>\n",
       "      <td>0.545338</td>\n",
       "      <td>-1.185844</td>\n",
       "      <td>-1.729828</td>\n",
       "      <td>2.932315</td>\n",
       "      <td>3.401529</td>\n",
       "      <td>0.337434</td>\n",
       "      <td>0.925377</td>\n",
       "      <td>-0.165663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.266113</td>\n",
       "      <td>-0.716336</td>\n",
       "      <td>0.108519</td>\n",
       "      <td>0.688519</td>\n",
       "      <td>-0.460220</td>\n",
       "      <td>0.161939</td>\n",
       "      <td>0.265368</td>\n",
       "      <td>0.090245</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284786</th>\n",
       "      <td>172768.0</td>\n",
       "      <td>-2.076175</td>\n",
       "      <td>2.142238</td>\n",
       "      <td>-2.522704</td>\n",
       "      <td>-1.888063</td>\n",
       "      <td>1.982785</td>\n",
       "      <td>3.732950</td>\n",
       "      <td>-1.217430</td>\n",
       "      <td>-0.536644</td>\n",
       "      <td>0.272867</td>\n",
       "      <td>...</td>\n",
       "      <td>2.016666</td>\n",
       "      <td>-1.588269</td>\n",
       "      <td>0.588482</td>\n",
       "      <td>0.632444</td>\n",
       "      <td>-0.201064</td>\n",
       "      <td>0.199251</td>\n",
       "      <td>0.438657</td>\n",
       "      <td>0.172923</td>\n",
       "      <td>8.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284787</th>\n",
       "      <td>172769.0</td>\n",
       "      <td>-1.029719</td>\n",
       "      <td>-1.110670</td>\n",
       "      <td>-0.636179</td>\n",
       "      <td>-0.840816</td>\n",
       "      <td>2.424360</td>\n",
       "      <td>-2.956733</td>\n",
       "      <td>0.283610</td>\n",
       "      <td>-0.332656</td>\n",
       "      <td>-0.247488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353722</td>\n",
       "      <td>0.488487</td>\n",
       "      <td>0.293632</td>\n",
       "      <td>0.107812</td>\n",
       "      <td>-0.935586</td>\n",
       "      <td>1.138216</td>\n",
       "      <td>0.025271</td>\n",
       "      <td>0.255347</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284788</th>\n",
       "      <td>172770.0</td>\n",
       "      <td>2.007418</td>\n",
       "      <td>-0.280235</td>\n",
       "      <td>-0.208113</td>\n",
       "      <td>0.335261</td>\n",
       "      <td>-0.715798</td>\n",
       "      <td>-0.751373</td>\n",
       "      <td>-0.458972</td>\n",
       "      <td>-0.140140</td>\n",
       "      <td>0.959971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208260</td>\n",
       "      <td>-0.430347</td>\n",
       "      <td>0.416765</td>\n",
       "      <td>0.064819</td>\n",
       "      <td>-0.608337</td>\n",
       "      <td>0.268436</td>\n",
       "      <td>-0.028069</td>\n",
       "      <td>-0.041367</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284789</th>\n",
       "      <td>172770.0</td>\n",
       "      <td>-0.446951</td>\n",
       "      <td>1.302212</td>\n",
       "      <td>-0.168583</td>\n",
       "      <td>0.981577</td>\n",
       "      <td>0.578957</td>\n",
       "      <td>-0.605641</td>\n",
       "      <td>1.253430</td>\n",
       "      <td>-1.042610</td>\n",
       "      <td>-0.417116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.851800</td>\n",
       "      <td>0.305268</td>\n",
       "      <td>-0.148093</td>\n",
       "      <td>-0.038712</td>\n",
       "      <td>0.010209</td>\n",
       "      <td>-0.362666</td>\n",
       "      <td>0.503092</td>\n",
       "      <td>0.229921</td>\n",
       "      <td>60.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284790</th>\n",
       "      <td>172771.0</td>\n",
       "      <td>-0.515513</td>\n",
       "      <td>0.971950</td>\n",
       "      <td>-1.014580</td>\n",
       "      <td>-0.677037</td>\n",
       "      <td>0.912430</td>\n",
       "      <td>-0.316187</td>\n",
       "      <td>0.396137</td>\n",
       "      <td>0.532364</td>\n",
       "      <td>-0.224606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280302</td>\n",
       "      <td>-0.849919</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>-0.376379</td>\n",
       "      <td>0.128660</td>\n",
       "      <td>-0.015205</td>\n",
       "      <td>-0.021486</td>\n",
       "      <td>9.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284791</th>\n",
       "      <td>172774.0</td>\n",
       "      <td>-0.863506</td>\n",
       "      <td>0.874701</td>\n",
       "      <td>0.420358</td>\n",
       "      <td>-0.530365</td>\n",
       "      <td>0.356561</td>\n",
       "      <td>-1.046238</td>\n",
       "      <td>0.757051</td>\n",
       "      <td>0.230473</td>\n",
       "      <td>-0.506856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108846</td>\n",
       "      <td>-0.480820</td>\n",
       "      <td>-0.074513</td>\n",
       "      <td>-0.003988</td>\n",
       "      <td>-0.113149</td>\n",
       "      <td>0.280378</td>\n",
       "      <td>-0.077310</td>\n",
       "      <td>0.023079</td>\n",
       "      <td>20.32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284792</th>\n",
       "      <td>172774.0</td>\n",
       "      <td>-0.724123</td>\n",
       "      <td>1.485216</td>\n",
       "      <td>-1.132218</td>\n",
       "      <td>-0.607190</td>\n",
       "      <td>0.709499</td>\n",
       "      <td>-0.482638</td>\n",
       "      <td>0.548393</td>\n",
       "      <td>0.343003</td>\n",
       "      <td>-0.226323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414621</td>\n",
       "      <td>1.307511</td>\n",
       "      <td>-0.059545</td>\n",
       "      <td>0.242669</td>\n",
       "      <td>-0.665424</td>\n",
       "      <td>-0.269869</td>\n",
       "      <td>-0.170579</td>\n",
       "      <td>-0.030692</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284793</th>\n",
       "      <td>172775.0</td>\n",
       "      <td>1.971002</td>\n",
       "      <td>-0.699067</td>\n",
       "      <td>-1.697541</td>\n",
       "      <td>-0.617643</td>\n",
       "      <td>1.718797</td>\n",
       "      <td>3.911336</td>\n",
       "      <td>-1.259306</td>\n",
       "      <td>1.056209</td>\n",
       "      <td>1.315006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188758</td>\n",
       "      <td>0.694418</td>\n",
       "      <td>0.163002</td>\n",
       "      <td>0.726365</td>\n",
       "      <td>-0.058282</td>\n",
       "      <td>-0.191813</td>\n",
       "      <td>0.061858</td>\n",
       "      <td>-0.043716</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284794</th>\n",
       "      <td>172777.0</td>\n",
       "      <td>-1.266580</td>\n",
       "      <td>-0.400461</td>\n",
       "      <td>0.956221</td>\n",
       "      <td>-0.723919</td>\n",
       "      <td>1.531993</td>\n",
       "      <td>-1.788600</td>\n",
       "      <td>0.314741</td>\n",
       "      <td>0.004704</td>\n",
       "      <td>0.013857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157831</td>\n",
       "      <td>-0.883365</td>\n",
       "      <td>0.088485</td>\n",
       "      <td>-0.076790</td>\n",
       "      <td>-0.095833</td>\n",
       "      <td>0.132720</td>\n",
       "      <td>-0.028468</td>\n",
       "      <td>0.126494</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284795</th>\n",
       "      <td>172778.0</td>\n",
       "      <td>-12.516732</td>\n",
       "      <td>10.187818</td>\n",
       "      <td>-8.476671</td>\n",
       "      <td>-2.510473</td>\n",
       "      <td>-4.586669</td>\n",
       "      <td>-1.394465</td>\n",
       "      <td>-3.632516</td>\n",
       "      <td>5.498583</td>\n",
       "      <td>4.893089</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.944759</td>\n",
       "      <td>-1.565026</td>\n",
       "      <td>0.890675</td>\n",
       "      <td>-1.253276</td>\n",
       "      <td>1.786717</td>\n",
       "      <td>0.320763</td>\n",
       "      <td>2.090712</td>\n",
       "      <td>1.232864</td>\n",
       "      <td>9.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284796</th>\n",
       "      <td>172780.0</td>\n",
       "      <td>1.884849</td>\n",
       "      <td>-0.143540</td>\n",
       "      <td>-0.999943</td>\n",
       "      <td>1.506772</td>\n",
       "      <td>-0.035300</td>\n",
       "      <td>-0.613638</td>\n",
       "      <td>0.190241</td>\n",
       "      <td>-0.249058</td>\n",
       "      <td>0.666458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144008</td>\n",
       "      <td>0.634646</td>\n",
       "      <td>-0.042114</td>\n",
       "      <td>-0.053206</td>\n",
       "      <td>0.316403</td>\n",
       "      <td>-0.461441</td>\n",
       "      <td>0.018265</td>\n",
       "      <td>-0.041068</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284797</th>\n",
       "      <td>172782.0</td>\n",
       "      <td>-0.241923</td>\n",
       "      <td>0.712247</td>\n",
       "      <td>0.399806</td>\n",
       "      <td>-0.463406</td>\n",
       "      <td>0.244531</td>\n",
       "      <td>-1.343668</td>\n",
       "      <td>0.929369</td>\n",
       "      <td>-0.206210</td>\n",
       "      <td>0.106234</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228876</td>\n",
       "      <td>-0.514376</td>\n",
       "      <td>0.279598</td>\n",
       "      <td>0.371441</td>\n",
       "      <td>-0.559238</td>\n",
       "      <td>0.113144</td>\n",
       "      <td>0.131507</td>\n",
       "      <td>0.081265</td>\n",
       "      <td>5.49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284798</th>\n",
       "      <td>172782.0</td>\n",
       "      <td>0.219529</td>\n",
       "      <td>0.881246</td>\n",
       "      <td>-0.635891</td>\n",
       "      <td>0.960928</td>\n",
       "      <td>-0.152971</td>\n",
       "      <td>-1.014307</td>\n",
       "      <td>0.427126</td>\n",
       "      <td>0.121340</td>\n",
       "      <td>-0.285670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099936</td>\n",
       "      <td>0.337120</td>\n",
       "      <td>0.251791</td>\n",
       "      <td>0.057688</td>\n",
       "      <td>-1.508368</td>\n",
       "      <td>0.144023</td>\n",
       "      <td>0.181205</td>\n",
       "      <td>0.215243</td>\n",
       "      <td>24.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284799</th>\n",
       "      <td>172783.0</td>\n",
       "      <td>-1.775135</td>\n",
       "      <td>-0.004235</td>\n",
       "      <td>1.189786</td>\n",
       "      <td>0.331096</td>\n",
       "      <td>1.196063</td>\n",
       "      <td>5.519980</td>\n",
       "      <td>-1.518185</td>\n",
       "      <td>2.080825</td>\n",
       "      <td>1.159498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103302</td>\n",
       "      <td>0.654850</td>\n",
       "      <td>-0.348929</td>\n",
       "      <td>0.745323</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>-0.127579</td>\n",
       "      <td>0.454379</td>\n",
       "      <td>0.130308</td>\n",
       "      <td>79.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284800</th>\n",
       "      <td>172784.0</td>\n",
       "      <td>2.039560</td>\n",
       "      <td>-0.175233</td>\n",
       "      <td>-1.196825</td>\n",
       "      <td>0.234580</td>\n",
       "      <td>-0.008713</td>\n",
       "      <td>-0.726571</td>\n",
       "      <td>0.017050</td>\n",
       "      <td>-0.118228</td>\n",
       "      <td>0.435402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.268048</td>\n",
       "      <td>-0.717211</td>\n",
       "      <td>0.297930</td>\n",
       "      <td>-0.359769</td>\n",
       "      <td>-0.315610</td>\n",
       "      <td>0.201114</td>\n",
       "      <td>-0.080826</td>\n",
       "      <td>-0.075071</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284801</th>\n",
       "      <td>172785.0</td>\n",
       "      <td>0.120316</td>\n",
       "      <td>0.931005</td>\n",
       "      <td>-0.546012</td>\n",
       "      <td>-0.745097</td>\n",
       "      <td>1.130314</td>\n",
       "      <td>-0.235973</td>\n",
       "      <td>0.812722</td>\n",
       "      <td>0.115093</td>\n",
       "      <td>-0.204064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314205</td>\n",
       "      <td>-0.808520</td>\n",
       "      <td>0.050343</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>-0.435870</td>\n",
       "      <td>0.124079</td>\n",
       "      <td>0.217940</td>\n",
       "      <td>0.068803</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "5            2.0  -0.425966   0.960523  1.141109 -0.168252  0.420987   \n",
       "6            4.0   1.229658   0.141004  0.045371  1.202613  0.191881   \n",
       "7            7.0  -0.644269   1.417964  1.074380 -0.492199  0.948934   \n",
       "8            7.0  -0.894286   0.286157 -0.113192 -0.271526  2.669599   \n",
       "9            9.0  -0.338262   1.119593  1.044367 -0.222187  0.499361   \n",
       "10          10.0   1.449044  -1.176339  0.913860 -1.375667 -1.971383   \n",
       "11          10.0   0.384978   0.616109 -0.874300 -0.094019  2.924584   \n",
       "12          10.0   1.249999  -1.221637  0.383930 -1.234899 -1.485419   \n",
       "13          11.0   1.069374   0.287722  0.828613  2.712520 -0.178398   \n",
       "14          12.0  -2.791855  -0.327771  1.641750  1.767473 -0.136588   \n",
       "15          12.0  -0.752417   0.345485  2.057323 -1.468643 -1.158394   \n",
       "16          12.0   1.103215  -0.040296  1.267332  1.289091 -0.735997   \n",
       "17          13.0  -0.436905   0.918966  0.924591 -0.727219  0.915679   \n",
       "18          14.0  -5.401258  -5.450148  1.186305  1.736239  3.049106   \n",
       "19          15.0   1.492936  -1.029346  0.454795 -1.438026 -1.555434   \n",
       "20          16.0   0.694885  -1.361819  1.029221  0.834159 -1.191209   \n",
       "21          17.0   0.962496   0.328461 -0.171479  2.109204  1.129566   \n",
       "22          18.0   1.166616   0.502120 -0.067300  2.261569  0.428804   \n",
       "23          18.0   0.247491   0.277666  1.185471 -0.092603 -1.314394   \n",
       "24          22.0  -1.946525  -0.044901 -0.405570 -1.013057  2.941968   \n",
       "25          22.0  -2.074295  -0.121482  1.322021  0.410008  0.295198   \n",
       "26          23.0   1.173285   0.353498  0.283905  1.133563 -0.172577   \n",
       "27          23.0   1.322707  -0.174041  0.434555  0.576038 -0.836758   \n",
       "28          23.0  -0.414289   0.905437  1.727453  1.473471  0.007443   \n",
       "29          23.0   1.059387  -0.175319  1.266130  1.186110 -0.786002   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284777  172764.0   2.079137  -0.028723 -1.343392  0.358000 -0.045791   \n",
       "284778  172764.0  -0.764523   0.588379 -0.907599 -0.418847  0.901528   \n",
       "284779  172766.0   1.975178  -0.616244 -2.628295 -0.406246  2.327804   \n",
       "284780  172766.0  -1.727503   1.108356  2.219561  1.148583 -0.884199   \n",
       "284781  172766.0  -1.139015  -0.155510  1.894478 -1.138957  1.451777   \n",
       "284782  172767.0  -0.268061   2.540315 -1.400915  4.846661  0.639105   \n",
       "284783  172768.0  -1.796092   1.929178 -2.828417 -1.689844  2.199572   \n",
       "284784  172768.0  -0.669662   0.923769 -1.543167 -1.560729  2.833960   \n",
       "284785  172768.0   0.032887   0.545338 -1.185844 -1.729828  2.932315   \n",
       "284786  172768.0  -2.076175   2.142238 -2.522704 -1.888063  1.982785   \n",
       "284787  172769.0  -1.029719  -1.110670 -0.636179 -0.840816  2.424360   \n",
       "284788  172770.0   2.007418  -0.280235 -0.208113  0.335261 -0.715798   \n",
       "284789  172770.0  -0.446951   1.302212 -0.168583  0.981577  0.578957   \n",
       "284790  172771.0  -0.515513   0.971950 -1.014580 -0.677037  0.912430   \n",
       "284791  172774.0  -0.863506   0.874701  0.420358 -0.530365  0.356561   \n",
       "284792  172774.0  -0.724123   1.485216 -1.132218 -0.607190  0.709499   \n",
       "284793  172775.0   1.971002  -0.699067 -1.697541 -0.617643  1.718797   \n",
       "284794  172777.0  -1.266580  -0.400461  0.956221 -0.723919  1.531993   \n",
       "284795  172778.0 -12.516732  10.187818 -8.476671 -2.510473 -4.586669   \n",
       "284796  172780.0   1.884849  -0.143540 -0.999943  1.506772 -0.035300   \n",
       "284797  172782.0  -0.241923   0.712247  0.399806 -0.463406  0.244531   \n",
       "284798  172782.0   0.219529   0.881246 -0.635891  0.960928 -0.152971   \n",
       "284799  172783.0  -1.775135  -0.004235  1.189786  0.331096  1.196063   \n",
       "284800  172784.0   2.039560  -0.175233 -1.196825  0.234580 -0.008713   \n",
       "284801  172785.0   0.120316   0.931005 -0.546012 -0.745097  1.130314   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "5      -0.029728  0.476201  0.260314 -0.568671  ... -0.208254 -0.559825   \n",
       "6       0.272708 -0.005159  0.081213  0.464960  ... -0.167716 -0.270710   \n",
       "7       0.428118  1.120631 -3.807864  0.615375  ...  1.943465 -1.015455   \n",
       "8       3.721818  0.370145  0.851084 -0.392048  ... -0.073425 -0.268092   \n",
       "9      -0.246761  0.651583  0.069539 -0.736727  ... -0.246914 -0.633753   \n",
       "10     -0.629152 -1.423236  0.048456 -1.720408  ... -0.009302  0.313894   \n",
       "11      3.317027  0.470455  0.538247 -0.558895  ...  0.049924  0.238422   \n",
       "12     -0.753230 -0.689405 -0.227487 -2.094011  ... -0.231809 -0.483285   \n",
       "13      0.337544 -0.096717  0.115982 -0.221083  ... -0.036876  0.074412   \n",
       "14      0.807596 -0.422911 -1.907107  0.755713  ...  1.151663  0.222182   \n",
       "15     -0.077850 -0.608581  0.003603 -0.436167  ...  0.499625  1.353650   \n",
       "16      0.288069 -0.586057  0.189380  0.782333  ... -0.024612  0.196002   \n",
       "17     -0.127867  0.707642  0.087962 -0.665271  ... -0.194796 -0.672638   \n",
       "18     -1.763406 -1.559738  0.160842  1.233090  ... -0.503600  0.984460   \n",
       "19     -0.720961 -1.080664 -0.053127 -1.978682  ... -0.177650 -0.175074   \n",
       "20      1.309109 -0.878586  0.445290 -0.446196  ... -0.295583 -0.571955   \n",
       "21      1.696038  0.107712  0.521502 -1.191311  ...  0.143997  0.402492   \n",
       "22      0.089474  0.241147  0.138082 -0.989162  ...  0.018702 -0.061972   \n",
       "23     -0.150116 -0.946365 -1.617935  1.544071  ...  1.650180  0.200454   \n",
       "24      2.955053 -0.063063  0.855546  0.049967  ... -0.579526 -0.799229   \n",
       "25     -0.959537  0.543985 -0.104627  0.475664  ... -0.403639 -0.227404   \n",
       "26     -0.916054  0.369025 -0.327260 -0.246651  ...  0.067003  0.227812   \n",
       "27     -0.831083 -0.264905 -0.220982 -1.071425  ... -0.284376 -0.323357   \n",
       "28     -0.200331  0.740228 -0.029247 -0.593392  ...  0.077237  0.457331   \n",
       "29      0.578435 -0.767084  0.401046  0.699500  ...  0.013676  0.213734   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284777 -1.345452  0.227476 -0.378355  0.665911  ...  0.235758  0.829758   \n",
       "284778 -0.760802  0.758545  0.414698 -0.730854  ...  0.003530 -0.431876   \n",
       "284779  3.664740 -0.533297  0.842937  1.128798  ...  0.086043  0.543613   \n",
       "284780  0.793083 -0.527298  0.866429  0.853819  ... -0.094708  0.236818   \n",
       "284781  0.093598  0.191353  0.092211 -0.062621  ... -0.191027 -0.631658   \n",
       "284782  0.186479 -0.045911  0.936448 -2.419986  ... -0.263889 -0.857904   \n",
       "284783  3.123732 -0.270714  1.657495  0.465804  ...  0.271170  1.145750   \n",
       "284784  3.240843  0.181576  1.282746 -0.893890  ...  0.183856  0.202670   \n",
       "284785  3.401529  0.337434  0.925377 -0.165663  ... -0.266113 -0.716336   \n",
       "284786  3.732950 -1.217430 -0.536644  0.272867  ...  2.016666 -1.588269   \n",
       "284787 -2.956733  0.283610 -0.332656 -0.247488  ...  0.353722  0.488487   \n",
       "284788 -0.751373 -0.458972 -0.140140  0.959971  ... -0.208260 -0.430347   \n",
       "284789 -0.605641  1.253430 -1.042610 -0.417116  ...  0.851800  0.305268   \n",
       "284790 -0.316187  0.396137  0.532364 -0.224606  ... -0.280302 -0.849919   \n",
       "284791 -1.046238  0.757051  0.230473 -0.506856  ... -0.108846 -0.480820   \n",
       "284792 -0.482638  0.548393  0.343003 -0.226323  ...  0.414621  1.307511   \n",
       "284793  3.911336 -1.259306  1.056209  1.315006  ...  0.188758  0.694418   \n",
       "284794 -1.788600  0.314741  0.004704  0.013857  ... -0.157831 -0.883365   \n",
       "284795 -1.394465 -3.632516  5.498583  4.893089  ... -0.944759 -1.565026   \n",
       "284796 -0.613638  0.190241 -0.249058  0.666458  ...  0.144008  0.634646   \n",
       "284797 -1.343668  0.929369 -0.206210  0.106234  ... -0.228876 -0.514376   \n",
       "284798 -1.014307  0.427126  0.121340 -0.285670  ...  0.099936  0.337120   \n",
       "284799  5.519980 -1.518185  2.080825  1.159498  ...  0.103302  0.654850   \n",
       "284800 -0.726571  0.017050 -0.118228  0.435402  ... -0.268048 -0.717211   \n",
       "284801 -0.235973  0.812722  0.115093 -0.204064  ... -0.314205 -0.808520   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "5      -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080    3.67   \n",
       "6      -0.154104 -0.780055  0.750137 -0.257237  0.034507  0.005168    4.99   \n",
       "7       0.057504 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   40.80   \n",
       "8      -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404   93.20   \n",
       "9      -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076    3.68   \n",
       "10      0.027740  0.500512  0.251367 -0.129478  0.042850  0.016253    7.80   \n",
       "11      0.009130  0.996710 -0.767315 -0.492208  0.042472 -0.054337    9.99   \n",
       "12      0.084668  0.392831  0.161135 -0.354990  0.026416  0.042422  121.50   \n",
       "13     -0.071407  0.104744  0.548265  0.104094  0.021491  0.021293   27.50   \n",
       "14      1.020586  0.028317 -0.232746 -0.235557 -0.164778 -0.030154   58.80   \n",
       "15     -0.256573 -0.065084 -0.039124 -0.087086 -0.180998  0.129394   15.99   \n",
       "16      0.013802  0.103758  0.364298 -0.382261  0.092809  0.037051   12.99   \n",
       "17     -0.156858 -0.888386 -0.342413 -0.049027  0.079692  0.131024    0.89   \n",
       "18      2.458589  0.042119 -0.481631 -0.621272  0.392053  0.949594   46.80   \n",
       "19      0.040002  0.295814  0.332931 -0.220385  0.022298  0.007602    5.00   \n",
       "20     -0.050881 -0.304215  0.072001 -0.422234  0.086553  0.063499  231.71   \n",
       "21     -0.048508 -1.371866  0.390814  0.199964  0.016371 -0.014605   34.09   \n",
       "22     -0.103855 -0.370415  0.603200  0.108556 -0.040521 -0.011418    2.28   \n",
       "23     -0.185353  0.423073  0.820591 -0.227632  0.336634  0.250475   22.75   \n",
       "24      0.870300  0.983421  0.321201  0.149650  0.707519  0.014600    0.89   \n",
       "25      0.742435  0.398535  0.249212  0.274404  0.359969  0.243232   26.43   \n",
       "26     -0.150487  0.435045  0.724825 -0.337082  0.016368  0.030041   41.88   \n",
       "27     -0.037710  0.347151  0.559639 -0.280158  0.042335  0.028822   16.00   \n",
       "28     -0.038500  0.642522 -0.183891 -0.277464  0.182687  0.152665   33.00   \n",
       "29      0.014462  0.002951  0.294638 -0.395070  0.081461  0.024220   12.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284777 -0.002063  0.001344  0.262183 -0.105327 -0.022363 -0.060283    1.00   \n",
       "284778  0.141759  0.587119 -0.200998  0.267337 -0.152951 -0.065285   80.00   \n",
       "284779 -0.032129  0.768379  0.477688 -0.031833  0.014151 -0.066542   25.00   \n",
       "284780 -0.204280  1.158185  0.627801 -0.399981  0.510818  0.233265   30.00   \n",
       "284781 -0.147249  0.212931  0.354257 -0.241068 -0.161717 -0.149188   13.00   \n",
       "284782  0.235172 -0.681794 -0.668894  0.044657 -0.066751 -0.072447   12.82   \n",
       "284783  0.084783  0.721269 -0.529906 -0.240117  0.129126 -0.080620   11.46   \n",
       "284784 -0.373023  0.651122  1.073823  0.844590 -0.286676 -0.187719   40.00   \n",
       "284785  0.108519  0.688519 -0.460220  0.161939  0.265368  0.090245    1.79   \n",
       "284786  0.588482  0.632444 -0.201064  0.199251  0.438657  0.172923    8.95   \n",
       "284787  0.293632  0.107812 -0.935586  1.138216  0.025271  0.255347    9.99   \n",
       "284788  0.416765  0.064819 -0.608337  0.268436 -0.028069 -0.041367    3.99   \n",
       "284789 -0.148093 -0.038712  0.010209 -0.362666  0.503092  0.229921   60.50   \n",
       "284790  0.300245  0.000607 -0.376379  0.128660 -0.015205 -0.021486    9.81   \n",
       "284791 -0.074513 -0.003988 -0.113149  0.280378 -0.077310  0.023079   20.32   \n",
       "284792 -0.059545  0.242669 -0.665424 -0.269869 -0.170579 -0.030692    3.99   \n",
       "284793  0.163002  0.726365 -0.058282 -0.191813  0.061858 -0.043716    4.99   \n",
       "284794  0.088485 -0.076790 -0.095833  0.132720 -0.028468  0.126494    0.89   \n",
       "284795  0.890675 -1.253276  1.786717  0.320763  2.090712  1.232864    9.87   \n",
       "284796 -0.042114 -0.053206  0.316403 -0.461441  0.018265 -0.041068   60.00   \n",
       "284797  0.279598  0.371441 -0.559238  0.113144  0.131507  0.081265    5.49   \n",
       "284798  0.251791  0.057688 -1.508368  0.144023  0.181205  0.215243   24.05   \n",
       "284799 -0.348929  0.745323  0.704545 -0.127579  0.454379  0.130308   79.99   \n",
       "284800  0.297930 -0.359769 -0.315610  0.201114 -0.080826 -0.075071    2.68   \n",
       "284801  0.050343  0.102800 -0.435870  0.124079  0.217940  0.068803    2.69   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "5           0  \n",
       "6           0  \n",
       "7           0  \n",
       "8           0  \n",
       "9           0  \n",
       "10          0  \n",
       "11          0  \n",
       "12          0  \n",
       "13          0  \n",
       "14          0  \n",
       "15          0  \n",
       "16          0  \n",
       "17          0  \n",
       "18          0  \n",
       "19          0  \n",
       "20          0  \n",
       "21          0  \n",
       "22          0  \n",
       "23          0  \n",
       "24          0  \n",
       "25          0  \n",
       "26          0  \n",
       "27          0  \n",
       "28          0  \n",
       "29          0  \n",
       "...       ...  \n",
       "284777      0  \n",
       "284778      0  \n",
       "284779      0  \n",
       "284780      0  \n",
       "284781      0  \n",
       "284782      0  \n",
       "284783      0  \n",
       "284784      0  \n",
       "284785      0  \n",
       "284786      0  \n",
       "284787      0  \n",
       "284788      0  \n",
       "284789      0  \n",
       "284790      0  \n",
       "284791      0  \n",
       "284792      0  \n",
       "284793      0  \n",
       "284794      0  \n",
       "284795      0  \n",
       "284796      0  \n",
       "284797      0  \n",
       "284798      0  \n",
       "284799      0  \n",
       "284800      0  \n",
       "284801      0  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "path=\"Credit Card Fraud/creditcard.csv\"\n",
    "datos_df=pd.read_csv(path) \n",
    "datos_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Mediante filtros en la variable `Class` del DataFrame `datos_df`, obtén el número de: transacciones fraudulentas, transacciones no-fraudulentas, total de transacciones (cada entrada es una transacción). ¿Cuál es la proporción entre los miembros de las clases? ¿Las clases están balanceadas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. de Transacciones Fraudulentas 492\n",
      "No. de Transacciones No-Fraudulentas 284315\n",
      "No. Total de Transacciones 284807\n"
     ]
    }
   ],
   "source": [
    "print(\"No. de Transacciones Fraudulentas\",datos_df[datos_df['Class']==1].shape[0])\n",
    "print(\"No. de Transacciones No-Fraudulentas\",datos_df[datos_df['Class']==0].shape[0])\n",
    "print(\"No. Total de Transacciones\",datos_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advertencia.** Es recomendable no utilizar una SVM de esta librería con más de 100,000 datos. Utilizando la base completa de datos con validación cruzada de 5 cortes en una máquina de 8 Gb de RAM **el algoritmo tardó más de 12 horas en terminar**.\n",
    "\n",
    "Dado que el número total de datos es de casi 300,000, trabajaremos con un número considerablemente menor de datos: 10,000 (puedes modificar éste número según la capacidad de tu computadora, pero la idea es que los algoritmos no tarden más de 1 minuto en entrenar). Este número será fijo por el resto del problema.\n",
    "\n",
    "Hay varias formas de construir la submuestra. Compararemos 3 métodos diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. La siguiente celda toma los datos `X_data` y las etiquetas correspondientes `y_labels` para entrenar una SVM y obtener la matriz de confusión. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5307,   14],\n",
       "       [ 110, 4569]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc=SVC(C=1,kernel='rbf',gamma=\"auto\", coef0=0.0,class_weight='balanced',cache_size=500)\n",
    "svc.fit(X_data,y_labels)\n",
    "predicted_svm=svc.predict(X_data)\n",
    "conf_mat_svm=confusion_matrix(y_labels,predicted_svm,labels=[True,False]).T #False/0=Normal, True/1 = Anomalo\n",
    "conf_mat_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función toma como entrada la matriz de confusión y genera una tabla de distintas métricas de clasificación binaria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_conf_mat(matrix):\n",
    "    true_pos=matrix[0,0]\n",
    "    true_neg=matrix[1,1]\n",
    "    false_positive=matrix[0,1]\n",
    "    false_negative=matrix[1,0]\n",
    "    \n",
    "    total_pos=true_pos+false_negative\n",
    "    total_neg=true_neg+false_positive\n",
    "    \n",
    "    sensitivity=true_pos/total_pos\n",
    "    specificity=true_neg/total_neg\n",
    "    pos_predictive_value=true_pos/(true_pos+false_positive)\n",
    "    neg_predictive_value=true_neg/(true_neg+false_negative)\n",
    "    \n",
    "    false_neg_rate=1-sensitivity\n",
    "    false_pos_rate=1-specificity\n",
    "    \n",
    "    false_discovery_rate=1-pos_predictive_value\n",
    "    false_omission_rate=1-neg_predictive_value\n",
    "    \n",
    "    detection_rate=true_pos/(total_pos+total_neg)\n",
    "    omission_rate=true_neg/(total_pos+total_neg)\n",
    "    \n",
    "    accuracy=(true_pos+true_neg)/(total_pos+total_neg) \n",
    "    prevalence=(true_pos+false_negative)/(total_pos+total_neg)\n",
    "    \n",
    "    f1=2*(sensitivity*pos_predictive_value)/(sensitivity+pos_predictive_value)\n",
    "    \n",
    "    informedness= sensitivity + specificity - 1\n",
    "    markedness= pos_predictive_value + neg_predictive_value - 1\n",
    "    \n",
    "    a=(true_pos+false_positive)*(true_pos+false_negative)\n",
    "    b=(true_neg+false_positive)*(true_neg+false_negative)\n",
    "    mathews_cc=(true_pos*true_neg-false_positive*false_negative)/(np.sqrt(a)*np.sqrt(b))\n",
    "    \n",
    "    resumen1=[total_pos,total_neg,sensitivity,specificity,pos_predictive_value,neg_predictive_value]\n",
    "    resumen2=[false_neg_rate,false_pos_rate,false_discovery_rate,false_omission_rate,detection_rate,omission_rate]\n",
    "    resumen3=[accuracy,prevalence,f1,informedness,markedness,mathews_cc]\n",
    "    resumen=resumen1+resumen2+resumen3\n",
    "    \n",
    "    indices1=[\"Total Positive\",\"Total Negative\",\"Sensitivity\",\"Specificity\",\"Positive Predictive Value\",\"Neg Predictive Value\"]\n",
    "    indices2=[\"False Negative Rate\",\"False Positive Rate\",\"False Discovery Rate\",\"False Omission Rate\",\"Detection Rate\",\"Omission Rate\"]\n",
    "    indices3=[\"Accuracy\",\"Prevalence\",\"f1\",\"Informedness\",\"Markedness\",\"Mathews Correlation Coefficient\"]\n",
    "    indices=indices1+indices2+indices3\n",
    "    \n",
    "    stats=pd.DataFrame(resumen)\n",
    "    stats.columns=['cantidad']\n",
    "    stats.index=indices\n",
    "    stats.index.name='indicador'\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. a) Selecciona los primeros n=10,000 de la base de datos, separa las variables (features) de las etiquetas (labels) de la misma forma que se hizo en clase, nombra las variables `X_data` y `y_labels`. Entrena una SVM con los mismos parámetros de arriba, obtén la matriz de confusión y calcula las métricas usando la función `evaluate_conf_mat` que acabamos de definir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separaremos las etiquetas de los datos\n",
    "sample_df=datos_df.iloc[:10000,:]\n",
    "X_data=sample_df.iloc[:,1:-1].values\n",
    "y_labels=sample_df.iloc[:,-1].values #convierte los datos a arreglos de numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  38    0]\n",
      " [   0 9962]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cantidad</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indicador</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total Positive</th>\n",
       "      <td>38.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Negative</th>\n",
       "      <td>9962.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive Predictive Value</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neg Predictive Value</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Negative Rate</th>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Positive Rate</th>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Discovery Rate</th>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Omission Rate</th>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Detection Rate</th>\n",
       "      <td>0.0038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Omission Rate</th>\n",
       "      <td>0.9962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prevalence</th>\n",
       "      <td>0.0038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Informedness</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Markedness</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mathews Correlation Coefficient</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  cantidad\n",
       "indicador                                 \n",
       "Total Positive                     38.0000\n",
       "Total Negative                   9962.0000\n",
       "Sensitivity                         1.0000\n",
       "Specificity                         1.0000\n",
       "Positive Predictive Value           1.0000\n",
       "Neg Predictive Value                1.0000\n",
       "False Negative Rate                 0.0000\n",
       "False Positive Rate                 0.0000\n",
       "False Discovery Rate                0.0000\n",
       "False Omission Rate                 0.0000\n",
       "Detection Rate                      0.0038\n",
       "Omission Rate                       0.9962\n",
       "Accuracy                            1.0000\n",
       "Prevalence                          0.0038\n",
       "f1                                  1.0000\n",
       "Informedness                        1.0000\n",
       "Markedness                          1.0000\n",
       "Mathews Correlation Coefficient     1.0000"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc=SVC(C=1,kernel='rbf',gamma=\"auto\", coef0=0.0,class_weight='balanced',cache_size=500)\n",
    "svc.fit(X_data,y_labels)\n",
    "predicted_svm=svc.predict(X_data)\n",
    "conf_mat_svm=confusion_matrix(y_labels,predicted_svm,labels=[True,False]).T #False/0=Normal, True/1 = Anomalo\n",
    "print(conf_mat_svm)\n",
    "evaluate_conf_mat(conf_mat_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. b) Repite el inciso a) con los mismos parámetros para la SVM, pero seleccionando una muestra aleatoria de n=10,000 datos de la base total. Hint: utiliza la función `sample` de pandas. [[link]](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html) Selecciona una semilla o estado aleatorio para poder reproducir los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separaremos las etiquetas de los datos\n",
    "sample_df=datos_df.sample(n=10000,random_state=42)\n",
    "X_data=sample_df.iloc[:,1:-1].values\n",
    "y_labels=sample_df.iloc[:,-1].values #convierte los datos a arreglos de numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  16    0]\n",
      " [   0 9984]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cantidad</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indicador</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total Positive</th>\n",
       "      <td>16.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Negative</th>\n",
       "      <td>9984.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive Predictive Value</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neg Predictive Value</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Negative Rate</th>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Positive Rate</th>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Discovery Rate</th>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Omission Rate</th>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Detection Rate</th>\n",
       "      <td>0.0016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Omission Rate</th>\n",
       "      <td>0.9984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prevalence</th>\n",
       "      <td>0.0016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Informedness</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Markedness</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mathews Correlation Coefficient</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  cantidad\n",
       "indicador                                 \n",
       "Total Positive                     16.0000\n",
       "Total Negative                   9984.0000\n",
       "Sensitivity                         1.0000\n",
       "Specificity                         1.0000\n",
       "Positive Predictive Value           1.0000\n",
       "Neg Predictive Value                1.0000\n",
       "False Negative Rate                 0.0000\n",
       "False Positive Rate                 0.0000\n",
       "False Discovery Rate                0.0000\n",
       "False Omission Rate                 0.0000\n",
       "Detection Rate                      0.0016\n",
       "Omission Rate                       0.9984\n",
       "Accuracy                            1.0000\n",
       "Prevalence                          0.0016\n",
       "f1                                  1.0000\n",
       "Informedness                        1.0000\n",
       "Markedness                          1.0000\n",
       "Mathews Correlation Coefficient     1.0000"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc=SVC(C=1,kernel='rbf',gamma=\"auto\", coef0=0.0,class_weight='balanced',cache_size=500)\n",
    "svc.fit(X_data,y_labels)\n",
    "predicted_svm=svc.predict(X_data)\n",
    "conf_mat_svm=confusion_matrix(y_labels,predicted_svm,labels=[True,False]).T #False/0=Normal, True/1 = Anomalo\n",
    "print(conf_mat_svm)\n",
    "evaluate_conf_mat(conf_mat_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. c) Repite el inciso a) con los mismos parámetros para la SVM. Ahora deseamos tener una muestra balanceada de n=10,000 datos. Para esto selecciona los datos anómalos y copialos hasta tener aproximadamente 5,000 datos. Luego selecciona el número restante de datos no-anómalos para tener n datos en total. Hint: utiliza la función `concat` de pandas con una lista de dataframes. [[link]](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 31)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_fraud=datos_df[datos_df['Class']==1].shape[0]\n",
    "n_copies=int(5000/n_fraud)+1\n",
    "lista_fraud_dfs=[]\n",
    "\n",
    "for i in range(0,n_copies):\n",
    "    lista_fraud_dfs.append(datos_df[datos_df['Class']==1])\n",
    "print(len(lista_fraud_dfs))\n",
    "\n",
    "sample_df=pd.concat(lista_fraud_dfs)\n",
    "\n",
    "n_normal=10000-len(sample_df)\n",
    "sample_df=pd.concat([sample_df,datos_df.sample(n=n_normal,random_state=42)])\n",
    "sample_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data=sample_df.iloc[:,1:-1].values\n",
    "y_labels=sample_df.iloc[:,-1].values #convierte los datos a arreglos de numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5362    6]\n",
      " [  55 4577]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cantidad</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indicador</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total Positive</th>\n",
       "      <td>5417.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Negative</th>\n",
       "      <td>4583.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>0.989847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.998691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive Predictive Value</th>\n",
       "      <td>0.998882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neg Predictive Value</th>\n",
       "      <td>0.988126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Negative Rate</th>\n",
       "      <td>0.010153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Positive Rate</th>\n",
       "      <td>0.001309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Discovery Rate</th>\n",
       "      <td>0.001118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Omission Rate</th>\n",
       "      <td>0.011874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Detection Rate</th>\n",
       "      <td>0.536200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Omission Rate</th>\n",
       "      <td>0.457700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.993900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prevalence</th>\n",
       "      <td>0.541700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.994344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Informedness</th>\n",
       "      <td>0.988538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Markedness</th>\n",
       "      <td>0.987008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mathews Correlation Coefficient</th>\n",
       "      <td>0.987773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    cantidad\n",
       "indicador                                   \n",
       "Total Positive                   5417.000000\n",
       "Total Negative                   4583.000000\n",
       "Sensitivity                         0.989847\n",
       "Specificity                         0.998691\n",
       "Positive Predictive Value           0.998882\n",
       "Neg Predictive Value                0.988126\n",
       "False Negative Rate                 0.010153\n",
       "False Positive Rate                 0.001309\n",
       "False Discovery Rate                0.001118\n",
       "False Omission Rate                 0.011874\n",
       "Detection Rate                      0.536200\n",
       "Omission Rate                       0.457700\n",
       "Accuracy                            0.993900\n",
       "Prevalence                          0.541700\n",
       "f1                                  0.994344\n",
       "Informedness                        0.988538\n",
       "Markedness                          0.987008\n",
       "Mathews Correlation Coefficient     0.987773"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc=SVC(C=1,kernel='rbf',gamma=\"auto\", coef0=0.0,class_weight='balanced',cache_size=500)\n",
    "svc.fit(X_data,y_labels)\n",
    "predicted_svm=svc.predict(X_data)\n",
    "conf_mat_svm=confusion_matrix(y_labels,predicted_svm,labels=[True,False]).T #False/0=Normal, True/1 = Anomalo\n",
    "print(conf_mat_svm)\n",
    "evaluate_conf_mat(conf_mat_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. d) Compara las métricas obtenidas con los 3 métodos de los incisos a), b) y c) ¿Cuál crees que es mejor? ¿Por qué? Selecciona los datos de ese inciso para utilizarlos a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_df=sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Ahora separaremos los datos hallados en el inciso anterior en prueba y entrenamiento para evaluar el algorimo con datos que no había visto previamente, i.e. que tan bien se generaliza el modelo a datos nuevos. Utiliza los mismos parámetros para la SVM que en el inciso anterior.\n",
    "\n",
    "    a) Divide los datos en entrenamiento y prueba (70% y 30%), entrena el algoritmo con los datos de entrenamiento y obtén las métricas con los datos de prueba. ¿Cómo se comparan las métricas con las obtenidas en el mejor caso del inciso anterior? Hint: Utiliza la función `train_test_split`. [[link]](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_data=best_df.iloc[:,1:-1].values\n",
    "y_labels=best_df.iloc[:,-1].values #convierte los datos a arreglos de numpy\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data,y_labels,test_size=0.3,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1599   10]\n",
      " [  18 1373]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cantidad</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indicador</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total Positive</th>\n",
       "      <td>1617.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Negative</th>\n",
       "      <td>1383.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>0.988868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.992769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive Predictive Value</th>\n",
       "      <td>0.993785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neg Predictive Value</th>\n",
       "      <td>0.987060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Negative Rate</th>\n",
       "      <td>0.011132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Positive Rate</th>\n",
       "      <td>0.007231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Discovery Rate</th>\n",
       "      <td>0.006215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Omission Rate</th>\n",
       "      <td>0.012940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Detection Rate</th>\n",
       "      <td>0.533000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Omission Rate</th>\n",
       "      <td>0.457667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.990667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prevalence</th>\n",
       "      <td>0.539000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.991321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Informedness</th>\n",
       "      <td>0.981638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Markedness</th>\n",
       "      <td>0.980845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mathews Correlation Coefficient</th>\n",
       "      <td>0.981241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    cantidad\n",
       "indicador                                   \n",
       "Total Positive                   1617.000000\n",
       "Total Negative                   1383.000000\n",
       "Sensitivity                         0.988868\n",
       "Specificity                         0.992769\n",
       "Positive Predictive Value           0.993785\n",
       "Neg Predictive Value                0.987060\n",
       "False Negative Rate                 0.011132\n",
       "False Positive Rate                 0.007231\n",
       "False Discovery Rate                0.006215\n",
       "False Omission Rate                 0.012940\n",
       "Detection Rate                      0.533000\n",
       "Omission Rate                       0.457667\n",
       "Accuracy                            0.990667\n",
       "Prevalence                          0.539000\n",
       "f1                                  0.991321\n",
       "Informedness                        0.981638\n",
       "Markedness                          0.980845\n",
       "Mathews Correlation Coefficient     0.981241"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc=SVC(C=1,kernel='rbf',gamma=\"auto\", coef0=0.0,class_weight='balanced',cache_size=500)\n",
    "svc.fit(X_train,y_train)\n",
    "predicted_svm=svc.predict(X_test)\n",
    "conf_mat_svm=confusion_matrix(y_test,predicted_svm,labels=[True,False]).T #False/0=Normal, True/1 = Anomalo\n",
    "print(conf_mat_svm)\n",
    "evaluate_conf_mat(conf_mat_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. b) Ahora haremos la partición de prueba y entrenamiento mediante validación cruzada con 5 cortes (5-fold cross-validation). Utiliza la función `cross_val_predict` para entrenar el modelo y obtener las métricas de evaluación. [[link]](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[5361   24]\n",
      " [  56 4559]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cantidad</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indicador</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total Positive</th>\n",
       "      <td>5417.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Negative</th>\n",
       "      <td>4583.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>0.989662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.994763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive Predictive Value</th>\n",
       "      <td>0.995543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neg Predictive Value</th>\n",
       "      <td>0.987866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Negative Rate</th>\n",
       "      <td>0.010338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Positive Rate</th>\n",
       "      <td>0.005237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Discovery Rate</th>\n",
       "      <td>0.004457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Omission Rate</th>\n",
       "      <td>0.012134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Detection Rate</th>\n",
       "      <td>0.536100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Omission Rate</th>\n",
       "      <td>0.455900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.992000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prevalence</th>\n",
       "      <td>0.541700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.992594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Informedness</th>\n",
       "      <td>0.984425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Markedness</th>\n",
       "      <td>0.983409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mathews Correlation Coefficient</th>\n",
       "      <td>0.983917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    cantidad\n",
       "indicador                                   \n",
       "Total Positive                   5417.000000\n",
       "Total Negative                   4583.000000\n",
       "Sensitivity                         0.989662\n",
       "Specificity                         0.994763\n",
       "Positive Predictive Value           0.995543\n",
       "Neg Predictive Value                0.987866\n",
       "False Negative Rate                 0.010338\n",
       "False Positive Rate                 0.005237\n",
       "False Discovery Rate                0.004457\n",
       "False Omission Rate                 0.012134\n",
       "Detection Rate                      0.536100\n",
       "Omission Rate                       0.455900\n",
       "Accuracy                            0.992000\n",
       "Prevalence                          0.541700\n",
       "f1                                  0.992594\n",
       "Informedness                        0.984425\n",
       "Markedness                          0.983409\n",
       "Mathews Correlation Coefficient     0.983917"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "svc=SVC(C=1,kernel='rbf',gamma=\"auto\", coef0=0.0,class_weight='balanced',cache_size=400)\n",
    "predicted_svm=cross_val_predict(svc, X_data,y_labels, cv=5)\n",
    "conf_mat_svm=confusion_matrix(y_labels,predicted_svm,labels=[True,False]).T #False/0=Normal, True/1 = Anomalo\n",
    "print(\"\\n\",conf_mat_svm)\n",
    "evaluate_conf_mat(conf_mat_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. c) ¿Cómo se comparan las métricas obtenidas en los incisos a) y b)? ¿Qué modelo de los dos utilizarías para clasificar transacciones en la vida real? ¿Por qué? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Ahora optimizaremos los híperparámetros del algoritmo con el conjunto de validación, mediante validación cruzada en una malla de parámetros predeterminada.\n",
    "    \n",
    "    a) Divide los datos hallados en el inciso 2 en entrenamiento, validación y prueba (70%,15%, 15%). Hint: Utiliza la función `train_test_split` para dividir los datos entre prueba y el resto (X_rest, y_rest) y utilizala una segunda vez para dividirlos en validación y entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_data=best_df.iloc[:,1:-1].values\n",
    "y_labels=best_df.iloc[:,-1].values #convierte los datos a arreglos de numpy\n",
    "X_rest, X_test, y_rest, y_test = train_test_split(X_data,y_labels,test_size=0.15,shuffle=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_rest,y_rest,test_size=0.15/0.85,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7000, 29), (1500, 29), (1500, 29))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape,X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7000,), (1500,), (1500,))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape,y_test.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7, 0.15, 0.15)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape[0]/10000,y_test.shape[0]/10000,y_val.shape[0]/10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.    b) Utiliza ciclos `for` anidados para entrenar una SVM con todas las combinaciones posibles de valores de los híperparámetros que estan en el diccionario que aparece en la siguiente celda. Utiliza los datos de entrenamiento para entrenar y los datos de validación para calcular la métrica del Coeficiente de Correlación de Mathews. Escoge la combinación de híperparámetros que resulte en el mejor Coeficiente de Correlación de Mathews. Finalmente, con la combinación de parámetros elejida, utiliza los datos de prueba para calcular todas las métricas de evaluación con la función `eval_conf_mat`. \n",
    "\n",
    "Hint: Incluye el parámetro `max_iter=5000` dentro de la función `SVC`, para limitar el tiempo que tarda en converger cada interación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_grid={\"C\":[0.001,0.01,0.1,1,10,100,1000,10000],'class_weight':['balanced',None],\n",
    "          'kernel':['linear', 'poly', 'rbf', 'sigmoid']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 balanced linear 0.837804221737458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 balanced poly 0.02227943200217824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  del sys.path[0]\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 balanced rbf nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 balanced sigmoid 0.31574515609584963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 None linear 0.8292003353735918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 None poly -0.0015949613769508134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  del sys.path[0]\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 None rbf nan\n",
      "0.001 None sigmoid 0.4819383533484894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 balanced linear 0.8020914192659446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 balanced poly -0.01764468649611669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 balanced rbf 0.5499626002019276\n",
      "0.01 balanced sigmoid 0.13106558827016035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 None linear 0.8177155796278768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 None poly 0.13013703273643387\n",
      "0.01 None rbf 0.4809807556528692\n",
      "0.01 None sigmoid 0.10370510224276558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 balanced linear 0.6908263491961157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 balanced poly 0.18327496076333435\n",
      "0.1 balanced rbf 0.8130882555489155\n",
      "0.1 balanced sigmoid 0.1097855073508108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 None linear 0.547411475310619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 None poly 0.30825667692277803\n",
      "0.1 None rbf 0.8069590641219856\n",
      "0.1 None sigmoid 0.07128317169895076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 balanced linear 0.8463406903618276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 balanced poly 0.05401607939588484\n",
      "1 balanced rbf 0.9731635682086973\n",
      "1 balanced sigmoid 0.10954311913083663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 None linear 0.8593557195695166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 None poly 0.06039305726204564\n",
      "1 None rbf 0.9798375713683376\n",
      "1 None sigmoid 0.06836854010449693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 balanced linear 0.598335658236387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 balanced poly 0.2515107701153615\n",
      "10 balanced rbf 0.9825516564564466\n",
      "10 balanced sigmoid 0.10662377307272522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 None linear 0.21658587634642984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 None poly 0.14804760926035143\n",
      "10 None rbf 0.9825516564564466\n",
      "10 None sigmoid 0.06836854010449693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 balanced linear 0.5917393262403216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 balanced poly 0.043102685964216826\n",
      "100 balanced rbf 0.9866291274577874\n",
      "100 balanced sigmoid 0.10662377307272522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 None linear 0.5917393262403216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 None poly 0.043102685964216826\n",
      "100 None rbf 0.9866291274577874\n",
      "100 None sigmoid 0.06836854010449693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 balanced linear 0.5917393262403216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 balanced poly 0.043102685964216826\n",
      "1000 balanced rbf 0.9866291274577874\n",
      "1000 balanced sigmoid 0.10662377307272522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 None linear 0.5917393262403216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 None poly 0.043102685964216826\n",
      "1000 None rbf 0.9866291274577874\n",
      "1000 None sigmoid 0.06836854010449693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 balanced linear 0.5917393262403216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 balanced poly 0.043102685964216826\n",
      "10000 balanced rbf 0.9866291274577874\n",
      "10000 balanced sigmoid 0.10662377307272522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 None linear 0.5917393262403216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 None poly 0.043102685964216826\n",
      "10000 None rbf 0.9866291274577874\n",
      "10000 None sigmoid 0.06836854010449693\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "algorithms=[]\n",
    "\n",
    "for c in svm_grid[\"C\"]:\n",
    "    for weight in svm_grid[\"class_weight\"]:\n",
    "        for kernel in svm_grid['kernel']:\n",
    "            #print(c, weight,kernel)\n",
    "            svc=SVC(C=c,kernel=kernel,class_weight=weight,cache_size=500,gamma='auto',max_iter=5000)\n",
    "            svc.fit(X_train,y_train) #entrenamiento\n",
    "            algorithms.append(svc)\n",
    "            \n",
    "            predicted_val=svc.predict(X_val) #prediccion con datos de validación\n",
    "            conf_mat_svm=confusion_matrix(y_val,predicted_val,labels=[True,False]).T #False/0=Normal, True/1 = Anomalo\n",
    "            mcc=evaluate_conf_mat(conf_mat_svm).loc['Mathews Correlation Coefficient'].values[0]\n",
    "            print(c,weight,kernel,mcc)\n",
    "            results.append([c,weight,kernel,mcc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>kernel</th>\n",
       "      <th>Mathews CC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1000.000</td>\n",
       "      <td>None</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.986629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>100.000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.986629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>None</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.986629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>100.000</td>\n",
       "      <td>None</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.986629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.986629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1000.000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.986629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>10.000</td>\n",
       "      <td>None</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.982552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10.000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.982552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.000</td>\n",
       "      <td>None</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.979838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.973164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.000</td>\n",
       "      <td>None</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.859356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.846341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>balanced</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.837804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001</td>\n",
       "      <td>None</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.829200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.010</td>\n",
       "      <td>None</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.817716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            C class_weight  kernel  Mathews CC\n",
       "54   1000.000         None     rbf    0.986629\n",
       "42    100.000     balanced     rbf    0.986629\n",
       "62  10000.000         None     rbf    0.986629\n",
       "46    100.000         None     rbf    0.986629\n",
       "58  10000.000     balanced     rbf    0.986629\n",
       "50   1000.000     balanced     rbf    0.986629\n",
       "38     10.000         None     rbf    0.982552\n",
       "34     10.000     balanced     rbf    0.982552\n",
       "30      1.000         None     rbf    0.979838\n",
       "26      1.000     balanced     rbf    0.973164\n",
       "28      1.000         None  linear    0.859356\n",
       "24      1.000     balanced  linear    0.846341\n",
       "0       0.001     balanced  linear    0.837804\n",
       "4       0.001         None  linear    0.829200\n",
       "12      0.010         None  linear    0.817716"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df=pd.DataFrame(results)\n",
    "results_df.columns=[\"C\",\"class_weight\",\"kernel\",\"Mathews CC\"]\n",
    "results_df=results_df.sort_values(by=\"Mathews CC\",ascending=False) #debemos elegir el algoritmo con el mejor resultado\n",
    "results_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1000,\n",
       " 'cache_size': 500,\n",
       " 'class_weight': None,\n",
       " 'coef0': 0.0,\n",
       " 'decision_function_shape': 'ovr',\n",
       " 'degree': 3,\n",
       " 'gamma': 'auto',\n",
       " 'kernel': 'rbf',\n",
       " 'max_iter': 5000,\n",
       " 'probability': False,\n",
       " 'random_state': None,\n",
       " 'shrinking': True,\n",
       " 'tol': 0.001,\n",
       " 'verbose': False}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_alg_index=results_df.index[0]\n",
    "best_alg=algorithms[best_alg_index]\n",
    "best_alg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[813   6]\n",
      " [  0 681]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cantidad</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indicador</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total Positive</th>\n",
       "      <td>813.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Negative</th>\n",
       "      <td>687.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.991266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive Predictive Value</th>\n",
       "      <td>0.992674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neg Predictive Value</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Negative Rate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Positive Rate</th>\n",
       "      <td>0.008734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Discovery Rate</th>\n",
       "      <td>0.007326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Omission Rate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Detection Rate</th>\n",
       "      <td>0.542000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Omission Rate</th>\n",
       "      <td>0.454000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.996000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prevalence</th>\n",
       "      <td>0.542000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.996324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Informedness</th>\n",
       "      <td>0.991266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Markedness</th>\n",
       "      <td>0.992674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mathews Correlation Coefficient</th>\n",
       "      <td>0.991970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   cantidad\n",
       "indicador                                  \n",
       "Total Positive                   813.000000\n",
       "Total Negative                   687.000000\n",
       "Sensitivity                        1.000000\n",
       "Specificity                        0.991266\n",
       "Positive Predictive Value          0.992674\n",
       "Neg Predictive Value               1.000000\n",
       "False Negative Rate                0.000000\n",
       "False Positive Rate                0.008734\n",
       "False Discovery Rate               0.007326\n",
       "False Omission Rate                0.000000\n",
       "Detection Rate                     0.542000\n",
       "Omission Rate                      0.454000\n",
       "Accuracy                           0.996000\n",
       "Prevalence                         0.542000\n",
       "f1                                 0.996324\n",
       "Informedness                       0.991266\n",
       "Markedness                         0.992674\n",
       "Mathews Correlation Coefficient    0.991970"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_test=best_alg.predict(X_test) #prediccion con datos de prueba\n",
    "conf_mat_svm=confusion_matrix(y_test,predicted_test,labels=[True,False]).T #False/0=Normal, True/1 = Anomalo\n",
    "print(conf_mat_svm)\n",
    "evaluate_conf_mat(conf_mat_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. c) En el inciso anterior no utilizamos validación cruzada en la parte de entrenamiento. Repite el inciso a) pero utiliza la función `cross_val_predict` con X_rest y y_rest para dividir los datos usados en entrenamiento y validación. Escoge la combinación de híperparámetros que resulte en el mejor Coeficiente de Correlación de Mathews. Finalmente, con la combinación de parámetros elejida, utiliza los datos de prueba para calcular todas las métricas de evaluación con la función `eval_conf_mat`.\n",
    "\n",
    "    Nota: Una vez seleccionado el modelo con los mejores parámetros, este se debe volver a entrenar con con X_rest y y_rest ya que `cross_val_predict` no regresa modelos entrenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8500, 29), (1500, 29))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_data=best_df.iloc[:,1:-1].values\n",
    "y_labels=best_df.iloc[:,-1].values #convierte los datos a arreglos de numpy\n",
    "X_rest, X_test, y_rest, y_test = train_test_split(X_data,y_labels,test_size=0.15,shuffle=True)\n",
    "\n",
    "X_rest.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 balanced linear 0.7833504154644726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 balanced poly 0.10408842154175174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  del sys.path[0]\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 balanced rbf nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 balanced sigmoid 0.3167909500793967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 None linear 0.8474088148805397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 None poly 0.0842222329116071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  del sys.path[0]\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 None rbf nan\n",
      "0.001 None sigmoid 0.48077942170327204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 balanced linear 0.8287150752644011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 balanced poly 0.21077991204556293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 balanced rbf 0.5600999119767451\n",
      "0.01 balanced sigmoid 0.09705519425061658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 None linear 0.7218340263817781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 None poly 0.04744546588177117\n",
      "0.01 None rbf 0.4817105578371748\n",
      "0.01 None sigmoid 0.06491449958154907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 balanced linear 0.46963007806781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 balanced poly 0.05931565201062845\n",
      "0.1 balanced rbf 0.8227142027006036\n",
      "0.1 balanced sigmoid 0.0690814794533964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 None linear 0.6886914282848543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 None poly 0.13444135888683925\n",
      "0.1 None rbf 0.8123846287264576\n",
      "0.1 None sigmoid 0.037483632262914186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 balanced linear 0.5221635952644732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 balanced poly 0.12053019406215215\n",
      "1 balanced rbf 0.9780349393296234\n",
      "1 balanced sigmoid 0.06566069726603979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 None linear 0.5509399983565311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 None poly 0.15019075139722188\n",
      "1 None rbf 0.9794304848063948\n",
      "1 None sigmoid 0.03275367620390298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 balanced linear 0.6294372384365378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 balanced poly 0.043765603689242254\n",
      "10 balanced rbf 0.9907875281785401\n",
      "10 balanced sigmoid 0.06713790945703635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 None linear 0.3316083398725659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 None poly 0.08325468909332279\n",
      "10 None rbf 0.9907950498981293\n",
      "10 None sigmoid 0.033009613244455234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 balanced linear 0.2504757965908613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 balanced poly 0.16434489968827545\n",
      "100 balanced rbf 0.9910302012244693\n",
      "100 balanced sigmoid 0.06713790945703635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 None linear 0.2504757965908613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 None poly 0.1224511879912235\n",
      "100 None rbf 0.9907950498981293\n",
      "100 None sigmoid 0.033009613244455234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 balanced linear 0.2504757965908613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 balanced poly 0.20244141392738169\n",
      "1000 balanced rbf 0.9912653980896773\n",
      "1000 balanced sigmoid 0.06713790945703635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 None linear 0.2504757965908613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 None poly 0.20244141392738169\n",
      "1000 None rbf 0.9912653980896773\n",
      "1000 None sigmoid 0.033009613244455234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 balanced linear 0.2504757965908613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 balanced poly 0.20244141392738169\n",
      "10000 balanced rbf 0.9912653980896773\n",
      "10000 balanced sigmoid 0.06713790945703635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 None linear 0.2504757965908613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 None poly 0.20244141392738169\n",
      "10000 None rbf 0.9912653980896773\n",
      "10000 None sigmoid 0.033009613244455234\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "algorithms=[]\n",
    "\n",
    "for c in svm_grid[\"C\"]:\n",
    "    for weight in svm_grid[\"class_weight\"]:\n",
    "        for kernel in svm_grid['kernel']:\n",
    "            #print(c, weight,kernel)\n",
    "            svc=SVC(C=c,kernel=kernel,class_weight=weight,cache_size=500,gamma='auto',max_iter=5000)\n",
    "            predicted_cv=cross_val_predict(svc, X_rest,y_rest, cv=5) #entrenamiento y predicción sobre dato en validacion\n",
    "            algorithms.append(svc)\n",
    "    \n",
    "            conf_mat_svm=confusion_matrix(y_rest,predicted_cv,labels=[True,False]).T #False/0=Normal, True/1 = Anomalo\n",
    "            mcc=evaluate_conf_mat(conf_mat_svm).loc['Mathews Correlation Coefficient'].values[0]\n",
    "            print(c,weight,kernel,mcc)\n",
    "            results.append([c,weight,kernel,mcc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>kernel</th>\n",
       "      <th>Mathews CC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.991265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1000.000</td>\n",
       "      <td>None</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.991265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>None</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.991265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1000.000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.991265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>100.000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.991030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>100.000</td>\n",
       "      <td>None</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.990795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>10.000</td>\n",
       "      <td>None</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.990795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10.000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.990788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.000</td>\n",
       "      <td>None</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.979430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.978035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001</td>\n",
       "      <td>None</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.847409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.010</td>\n",
       "      <td>balanced</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.828715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.100</td>\n",
       "      <td>balanced</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.822714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.100</td>\n",
       "      <td>None</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.812385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>balanced</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.783350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            C class_weight  kernel  Mathews CC\n",
       "58  10000.000     balanced     rbf    0.991265\n",
       "54   1000.000         None     rbf    0.991265\n",
       "62  10000.000         None     rbf    0.991265\n",
       "50   1000.000     balanced     rbf    0.991265\n",
       "42    100.000     balanced     rbf    0.991030\n",
       "46    100.000         None     rbf    0.990795\n",
       "38     10.000         None     rbf    0.990795\n",
       "34     10.000     balanced     rbf    0.990788\n",
       "30      1.000         None     rbf    0.979430\n",
       "26      1.000     balanced     rbf    0.978035\n",
       "4       0.001         None  linear    0.847409\n",
       "8       0.010     balanced  linear    0.828715\n",
       "18      0.100     balanced     rbf    0.822714\n",
       "22      0.100         None     rbf    0.812385\n",
       "0       0.001     balanced  linear    0.783350"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df=pd.DataFrame(results)\n",
    "results_df.columns=[\"C\",\"class_weight\",\"kernel\",\"Mathews CC\"]\n",
    "results_df=results_df.sort_values(by=\"Mathews CC\",ascending=False) #debemos elegir el algoritmo con el mejor resultado\n",
    "results_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10000,\n",
       " 'cache_size': 500,\n",
       " 'class_weight': 'balanced',\n",
       " 'coef0': 0.0,\n",
       " 'decision_function_shape': 'ovr',\n",
       " 'degree': 3,\n",
       " 'gamma': 'auto',\n",
       " 'kernel': 'rbf',\n",
       " 'max_iter': 5000,\n",
       " 'probability': False,\n",
       " 'random_state': None,\n",
       " 'shrinking': True,\n",
       " 'tol': 0.001,\n",
       " 'verbose': False}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_alg_index=results_df.index[0]\n",
    "best_alg=algorithms[best_alg_index]\n",
    "best_params=best_alg.get_params()\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[814   2]\n",
      " [  0 684]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cantidad</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indicador</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total Positive</th>\n",
       "      <td>814.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Negative</th>\n",
       "      <td>686.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.997085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive Predictive Value</th>\n",
       "      <td>0.997549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neg Predictive Value</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Negative Rate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Positive Rate</th>\n",
       "      <td>0.002915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Discovery Rate</th>\n",
       "      <td>0.002451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Omission Rate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Detection Rate</th>\n",
       "      <td>0.542667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Omission Rate</th>\n",
       "      <td>0.456000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.998667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prevalence</th>\n",
       "      <td>0.542667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.998773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Informedness</th>\n",
       "      <td>0.997085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Markedness</th>\n",
       "      <td>0.997549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mathews Correlation Coefficient</th>\n",
       "      <td>0.997317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   cantidad\n",
       "indicador                                  \n",
       "Total Positive                   814.000000\n",
       "Total Negative                   686.000000\n",
       "Sensitivity                        1.000000\n",
       "Specificity                        0.997085\n",
       "Positive Predictive Value          0.997549\n",
       "Neg Predictive Value               1.000000\n",
       "False Negative Rate                0.000000\n",
       "False Positive Rate                0.002915\n",
       "False Discovery Rate               0.002451\n",
       "False Omission Rate                0.000000\n",
       "Detection Rate                     0.542667\n",
       "Omission Rate                      0.456000\n",
       "Accuracy                           0.998667\n",
       "Prevalence                         0.542667\n",
       "f1                                 0.998773\n",
       "Informedness                       0.997085\n",
       "Markedness                         0.997549\n",
       "Mathews Correlation Coefficient    0.997317"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc=SVC().set_params(**best_params)\n",
    "svc.fit(X_rest,y_rest)\n",
    "predicted_test=svc.predict(X_test)\n",
    "conf_mat_svm=confusion_matrix(y_test,predicted_test,labels=[True,False]).T #False/0=Normal, True/1 = Anomalo\n",
    "print(conf_mat_svm)\n",
    "evaluate_conf_mat(conf_mat_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 d) Compara las metricas obtenidas con el conjunto de prueba en los incisos a) y b). ¿Cuál tiene mejores métricas?¿Qué modelo de los dos utilizarías para clasificar transacciones en la vida real? ¿Por qué? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. La siguientes celdas utilizan la función `GridSearchCV` para combinar validación cruzada y búsqueda de parámetros en una sola celda.[[link]](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) Dada la malla de parámetros (un diccionario) se elije una de todas las combinaciones posibles y se hace validación cruzada con cada combinación. Utilizamos la función de sklearn que calcula el Coeficiente de Correlación de Matthews para evaluar cada predicción. Luego seleccionamos el algoritmo con el mejor resultado en la métrica según la combinación de parámetros y lo evaluamos utilizando el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:   35.1s\n",
      "[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=3)]: Done 320 out of 320 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "       estimator=SVC(C=1.0, cache_size=500, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=5000, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=False, n_jobs=3,\n",
       "       param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000], 'class_weight': ['balanced', None], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=make_scorer(matthews_corrcoef), verbose=2)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import matthews_corrcoef,make_scorer\n",
    "import numpy as np\n",
    "\n",
    "mcc=make_scorer(matthews_corrcoef,greater_is_better=True) #convierte la función a un objeto de scoring para optimizar\n",
    "svm_grid={\"C\":[0.001,0.01,0.1,1,10,100,1000,10000],'class_weight':['balanced',None],\n",
    "          'kernel':['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "\n",
    "# le damos parámetros fijos a SVC,ie no varian como los otros hiperparámetros en el diccionario\n",
    "svm_cv=GridSearchCV(SVC(cache_size=500,gamma='auto',max_iter=5000), svm_grid, scoring=mcc, n_jobs=3, iid=False, refit=True, cv=5,\n",
    "             verbose=2, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=True)\n",
    "svm_cv.fit(X_rest,y_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9912717649529281\n",
      "{'C': 1000, 'class_weight': 'balanced', 'kernel': 'rbf'}\n",
      "50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cantidad</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indicador</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total Positive</th>\n",
       "      <td>814.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Negative</th>\n",
       "      <td>686.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.997085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive Predictive Value</th>\n",
       "      <td>0.997549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neg Predictive Value</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Negative Rate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Positive Rate</th>\n",
       "      <td>0.002915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Discovery Rate</th>\n",
       "      <td>0.002451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Omission Rate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Detection Rate</th>\n",
       "      <td>0.542667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Omission Rate</th>\n",
       "      <td>0.456000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.998667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prevalence</th>\n",
       "      <td>0.542667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.998773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Informedness</th>\n",
       "      <td>0.997085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Markedness</th>\n",
       "      <td>0.997549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mathews Correlation Coefficient</th>\n",
       "      <td>0.997317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   cantidad\n",
       "indicador                                  \n",
       "Total Positive                   814.000000\n",
       "Total Negative                   686.000000\n",
       "Sensitivity                        1.000000\n",
       "Specificity                        0.997085\n",
       "Positive Predictive Value          0.997549\n",
       "Neg Predictive Value               1.000000\n",
       "False Negative Rate                0.000000\n",
       "False Positive Rate                0.002915\n",
       "False Discovery Rate               0.002451\n",
       "False Omission Rate                0.000000\n",
       "Detection Rate                     0.542667\n",
       "Omission Rate                      0.456000\n",
       "Accuracy                           0.998667\n",
       "Prevalence                         0.542667\n",
       "f1                                 0.998773\n",
       "Informedness                       0.997085\n",
       "Markedness                         0.997549\n",
       "Mathews Correlation Coefficient    0.997317"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(svm_cv.best_score_)\n",
    "print(svm_cv.best_params_)\n",
    "print(svm_cv.best_index_)\n",
    "\n",
    "predicted_test=svm_cv.best_estimator_.predict(X_test)\n",
    "conf_mat_svm=confusion_matrix(y_test,predicted_test,labels=[True,False]).T #False/0=Normal, True/1 = Anomalo\n",
    "evaluate_conf_mat(conf_mat_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Repite el proceso que se realiza en estas dos celdas con los otros algoritmos de clasificación que hemos visto: Decision Tree y Random Forest. Aquí se importan las funciones y se definen los diccionarios a utilizar. Una vez obtenidas las métricas con el conjunto de prueba compara los 3 algoritmos de clasificación.¿Cuál es el que da mejores resultados? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "dtree_grid={'criterion':['gini','entropy'],'max_depth':[2,5,7,10,15,20],\n",
    "           'min_samples_split':[2,4,7,10,15],'class_weight':['balanced',None]}\n",
    "forest_grid={'n_estimators':[2,5,10,40,75,150],'criterion':['gini','entropy'],\n",
    "             'max_depth':[2,5,7,10,15,20],'min_samples_split':[2,4,7,10,15],'class_weight':['balanced',None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  86 tasks      | elapsed:    3.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9816714235735489\n",
      "{'class_weight': None, 'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 2}\n",
      "115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done 600 out of 600 | elapsed:   21.9s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cantidad</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indicador</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total Positive</th>\n",
       "      <td>814.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Negative</th>\n",
       "      <td>686.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.973761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive Predictive Value</th>\n",
       "      <td>0.978365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neg Predictive Value</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Negative Rate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Positive Rate</th>\n",
       "      <td>0.026239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Discovery Rate</th>\n",
       "      <td>0.021635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Omission Rate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Detection Rate</th>\n",
       "      <td>0.542667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Omission Rate</th>\n",
       "      <td>0.445333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prevalence</th>\n",
       "      <td>0.542667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.989064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Informedness</th>\n",
       "      <td>0.973761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Markedness</th>\n",
       "      <td>0.978365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mathews Correlation Coefficient</th>\n",
       "      <td>0.976060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   cantidad\n",
       "indicador                                  \n",
       "Total Positive                   814.000000\n",
       "Total Negative                   686.000000\n",
       "Sensitivity                        1.000000\n",
       "Specificity                        0.973761\n",
       "Positive Predictive Value          0.978365\n",
       "Neg Predictive Value               1.000000\n",
       "False Negative Rate                0.000000\n",
       "False Positive Rate                0.026239\n",
       "False Discovery Rate               0.021635\n",
       "False Omission Rate                0.000000\n",
       "Detection Rate                     0.542667\n",
       "Omission Rate                      0.445333\n",
       "Accuracy                           0.988000\n",
       "Prevalence                         0.542667\n",
       "f1                                 0.989064\n",
       "Informedness                       0.973761\n",
       "Markedness                         0.978365\n",
       "Mathews Correlation Coefficient    0.976060"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree_cv=GridSearchCV(DecisionTreeClassifier(), \n",
    "                      dtree_grid, scoring=mcc, n_jobs=3, iid=False, refit=True, cv=5, \n",
    "                      verbose=2, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=True)\n",
    "dtree_cv.fit(X_rest,y_rest)\n",
    "print(dtree_cv.best_score_)\n",
    "print(dtree_cv.best_params_)\n",
    "print(dtree_cv.best_index_)\n",
    "\n",
    "predicted_test=dtree_cv.best_estimator_.predict(X_test)\n",
    "conf_mat_dtree=confusion_matrix(y_test,predicted_test,labels=[True,False]).T #False/0=Normal, True/1 = Anomalo\n",
    "evaluate_conf_mat(conf_mat_dtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=6)]: Done 150 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=6)]: Done 353 tasks      | elapsed:   28.6s\n",
      "[Parallel(n_jobs=6)]: Done 636 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=6)]: Done 1001 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=6)]: Done 1446 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=6)]: Done 1973 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=6)]: Done 2580 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=6)]: Done 3269 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=6)]: Done 3600 out of 3600 | elapsed:  8.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9992896235280779\n",
      "{'class_weight': None, 'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 4, 'n_estimators': 40}\n",
      "699\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cantidad</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indicador</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total Positive</th>\n",
       "      <td>814.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Negative</th>\n",
       "      <td>686.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive Predictive Value</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neg Predictive Value</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Negative Rate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Positive Rate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Discovery Rate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Omission Rate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Detection Rate</th>\n",
       "      <td>0.542667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Omission Rate</th>\n",
       "      <td>0.457333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prevalence</th>\n",
       "      <td>0.542667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Informedness</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Markedness</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mathews Correlation Coefficient</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   cantidad\n",
       "indicador                                  \n",
       "Total Positive                   814.000000\n",
       "Total Negative                   686.000000\n",
       "Sensitivity                        1.000000\n",
       "Specificity                        1.000000\n",
       "Positive Predictive Value          1.000000\n",
       "Neg Predictive Value               1.000000\n",
       "False Negative Rate                0.000000\n",
       "False Positive Rate                0.000000\n",
       "False Discovery Rate               0.000000\n",
       "False Omission Rate                0.000000\n",
       "Detection Rate                     0.542667\n",
       "Omission Rate                      0.457333\n",
       "Accuracy                           1.000000\n",
       "Prevalence                         0.542667\n",
       "f1                                 1.000000\n",
       "Informedness                       1.000000\n",
       "Markedness                         1.000000\n",
       "Mathews Correlation Coefficient    1.000000"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_cv=GridSearchCV(RandomForestClassifier(), \n",
    "                      forest_grid, scoring=mcc, n_jobs=6, iid=False, refit=True, cv=5, \n",
    "                      verbose=2, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=True)\n",
    "forest_cv.fit(X_rest,y_rest)\n",
    "print(forest_cv.best_score_)\n",
    "print(forest_cv.best_params_)\n",
    "print(forest_cv.best_index_)\n",
    "\n",
    "predicted_test=forest_cv.best_estimator_.predict(X_test)\n",
    "conf_mat_forest=confusion_matrix(y_test,predicted_test,labels=[True,False]).T #False/0=Normal, True/1 = Anomalo\n",
    "evaluate_conf_mat(conf_mat_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVM</th>\n",
       "      <th>DTree</th>\n",
       "      <th>RForest</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indicador</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total Positive</th>\n",
       "      <td>814.000000</td>\n",
       "      <td>814.000000</td>\n",
       "      <td>814.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Negative</th>\n",
       "      <td>686.000000</td>\n",
       "      <td>686.000000</td>\n",
       "      <td>686.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.997085</td>\n",
       "      <td>0.973761</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive Predictive Value</th>\n",
       "      <td>0.997549</td>\n",
       "      <td>0.978365</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neg Predictive Value</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Negative Rate</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Positive Rate</th>\n",
       "      <td>0.002915</td>\n",
       "      <td>0.026239</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Discovery Rate</th>\n",
       "      <td>0.002451</td>\n",
       "      <td>0.021635</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Omission Rate</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Detection Rate</th>\n",
       "      <td>0.542667</td>\n",
       "      <td>0.542667</td>\n",
       "      <td>0.542667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Omission Rate</th>\n",
       "      <td>0.456000</td>\n",
       "      <td>0.445333</td>\n",
       "      <td>0.457333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.998667</td>\n",
       "      <td>0.988000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prevalence</th>\n",
       "      <td>0.542667</td>\n",
       "      <td>0.542667</td>\n",
       "      <td>0.542667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.998773</td>\n",
       "      <td>0.989064</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Informedness</th>\n",
       "      <td>0.997085</td>\n",
       "      <td>0.973761</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Markedness</th>\n",
       "      <td>0.997549</td>\n",
       "      <td>0.978365</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mathews Correlation Coefficient</th>\n",
       "      <td>0.997317</td>\n",
       "      <td>0.976060</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        SVM       DTree     RForest\n",
       "indicador                                                          \n",
       "Total Positive                   814.000000  814.000000  814.000000\n",
       "Total Negative                   686.000000  686.000000  686.000000\n",
       "Sensitivity                        1.000000    1.000000    1.000000\n",
       "Specificity                        0.997085    0.973761    1.000000\n",
       "Positive Predictive Value          0.997549    0.978365    1.000000\n",
       "Neg Predictive Value               1.000000    1.000000    1.000000\n",
       "False Negative Rate                0.000000    0.000000    0.000000\n",
       "False Positive Rate                0.002915    0.026239    0.000000\n",
       "False Discovery Rate               0.002451    0.021635    0.000000\n",
       "False Omission Rate                0.000000    0.000000    0.000000\n",
       "Detection Rate                     0.542667    0.542667    0.542667\n",
       "Omission Rate                      0.456000    0.445333    0.457333\n",
       "Accuracy                           0.998667    0.988000    1.000000\n",
       "Prevalence                         0.542667    0.542667    0.542667\n",
       "f1                                 0.998773    0.989064    1.000000\n",
       "Informedness                       0.997085    0.973761    1.000000\n",
       "Markedness                         0.997549    0.978365    1.000000\n",
       "Mathews Correlation Coefficient    0.997317    0.976060    1.000000"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_svm=evaluate_conf_mat(conf_mat_svm)\n",
    "res_dtree=evaluate_conf_mat(conf_mat_dtree)\n",
    "res_forest=evaluate_conf_mat(conf_mat_forest)\n",
    "\n",
    "\n",
    "resumen=pd.concat([res_svm,res_dtree,res_forest], axis=1, join='outer', join_axes=None, ignore_index=False,\n",
    "          keys=None, levels=None, names=None, verify_integrity=False,copy=True)\n",
    "resumen.columns=[\"SVM\",\"DTree\",\"RForest\"]\n",
    "resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema 2: Regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veremos un problema de regresión. Trabajaremos con una base de datos que recopila diferentes características de una casa en King County, Seattle, incluyendo el precio en que fué vendida. Las ventas ocurrieron entre Mayo 2014 y Mayo 2015. [[link]](https://www.kaggle.com/harlfoxem/housesalesprediction)\n",
    "\n",
    "La descripción de las distintas variables la puedes encontrar en el archivo: `kc_house_description.txt`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7237550310</td>\n",
       "      <td>20140512T000000</td>\n",
       "      <td>1225000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.50</td>\n",
       "      <td>5420</td>\n",
       "      <td>101930</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3890</td>\n",
       "      <td>1530</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>98053</td>\n",
       "      <td>47.6561</td>\n",
       "      <td>-122.005</td>\n",
       "      <td>4760</td>\n",
       "      <td>101930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1321400060</td>\n",
       "      <td>20140627T000000</td>\n",
       "      <td>257500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1715</td>\n",
       "      <td>6819</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1715</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>98003</td>\n",
       "      <td>47.3097</td>\n",
       "      <td>-122.327</td>\n",
       "      <td>2238</td>\n",
       "      <td>6819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008000270</td>\n",
       "      <td>20150115T000000</td>\n",
       "      <td>291850.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1060</td>\n",
       "      <td>9711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1060</td>\n",
       "      <td>0</td>\n",
       "      <td>1963</td>\n",
       "      <td>0</td>\n",
       "      <td>98198</td>\n",
       "      <td>47.4095</td>\n",
       "      <td>-122.315</td>\n",
       "      <td>1650</td>\n",
       "      <td>9711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2414600126</td>\n",
       "      <td>20150415T000000</td>\n",
       "      <td>229500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1780</td>\n",
       "      <td>7470</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>730</td>\n",
       "      <td>1960</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5123</td>\n",
       "      <td>-122.337</td>\n",
       "      <td>1780</td>\n",
       "      <td>8113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3793500160</td>\n",
       "      <td>20150312T000000</td>\n",
       "      <td>323000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1890</td>\n",
       "      <td>6560</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1890</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.3684</td>\n",
       "      <td>-122.031</td>\n",
       "      <td>2390</td>\n",
       "      <td>7570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1736800520</td>\n",
       "      <td>20150403T000000</td>\n",
       "      <td>662500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3560</td>\n",
       "      <td>9796</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1860</td>\n",
       "      <td>1700</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98007</td>\n",
       "      <td>47.6007</td>\n",
       "      <td>-122.145</td>\n",
       "      <td>2210</td>\n",
       "      <td>8925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9212900260</td>\n",
       "      <td>20140527T000000</td>\n",
       "      <td>468000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1160</td>\n",
       "      <td>6000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>860</td>\n",
       "      <td>300</td>\n",
       "      <td>1942</td>\n",
       "      <td>0</td>\n",
       "      <td>98115</td>\n",
       "      <td>47.6900</td>\n",
       "      <td>-122.292</td>\n",
       "      <td>1330</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>114101516</td>\n",
       "      <td>20140528T000000</td>\n",
       "      <td>310000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1430</td>\n",
       "      <td>19901</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1430</td>\n",
       "      <td>0</td>\n",
       "      <td>1927</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7558</td>\n",
       "      <td>-122.229</td>\n",
       "      <td>1780</td>\n",
       "      <td>12697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6054650070</td>\n",
       "      <td>20141007T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1370</td>\n",
       "      <td>9680</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1370</td>\n",
       "      <td>0</td>\n",
       "      <td>1977</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6127</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1370</td>\n",
       "      <td>10208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1175000570</td>\n",
       "      <td>20150312T000000</td>\n",
       "      <td>530000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1810</td>\n",
       "      <td>4850</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1810</td>\n",
       "      <td>0</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>98107</td>\n",
       "      <td>47.6700</td>\n",
       "      <td>-122.394</td>\n",
       "      <td>1360</td>\n",
       "      <td>4850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9297300055</td>\n",
       "      <td>20150124T000000</td>\n",
       "      <td>650000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2950</td>\n",
       "      <td>5000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1980</td>\n",
       "      <td>970</td>\n",
       "      <td>1979</td>\n",
       "      <td>0</td>\n",
       "      <td>98126</td>\n",
       "      <td>47.5714</td>\n",
       "      <td>-122.375</td>\n",
       "      <td>2140</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1875500060</td>\n",
       "      <td>20140731T000000</td>\n",
       "      <td>395000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1890</td>\n",
       "      <td>14040</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1890</td>\n",
       "      <td>0</td>\n",
       "      <td>1994</td>\n",
       "      <td>0</td>\n",
       "      <td>98019</td>\n",
       "      <td>47.7277</td>\n",
       "      <td>-121.962</td>\n",
       "      <td>1890</td>\n",
       "      <td>14018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6865200140</td>\n",
       "      <td>20140529T000000</td>\n",
       "      <td>485000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1600</td>\n",
       "      <td>4300</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>1916</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6648</td>\n",
       "      <td>-122.343</td>\n",
       "      <td>1610</td>\n",
       "      <td>4300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>16000397</td>\n",
       "      <td>20141205T000000</td>\n",
       "      <td>189000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1200</td>\n",
       "      <td>9850</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "      <td>1921</td>\n",
       "      <td>0</td>\n",
       "      <td>98002</td>\n",
       "      <td>47.3089</td>\n",
       "      <td>-122.210</td>\n",
       "      <td>1060</td>\n",
       "      <td>5095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7983200060</td>\n",
       "      <td>20150424T000000</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1250</td>\n",
       "      <td>9774</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1250</td>\n",
       "      <td>0</td>\n",
       "      <td>1969</td>\n",
       "      <td>0</td>\n",
       "      <td>98003</td>\n",
       "      <td>47.3343</td>\n",
       "      <td>-122.306</td>\n",
       "      <td>1280</td>\n",
       "      <td>8850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6300500875</td>\n",
       "      <td>20140514T000000</td>\n",
       "      <td>385000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1620</td>\n",
       "      <td>4980</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>860</td>\n",
       "      <td>760</td>\n",
       "      <td>1947</td>\n",
       "      <td>0</td>\n",
       "      <td>98133</td>\n",
       "      <td>47.7025</td>\n",
       "      <td>-122.341</td>\n",
       "      <td>1400</td>\n",
       "      <td>4980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2524049179</td>\n",
       "      <td>20140826T000000</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3050</td>\n",
       "      <td>44867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2330</td>\n",
       "      <td>720</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "      <td>98040</td>\n",
       "      <td>47.5316</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>4110</td>\n",
       "      <td>20336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7137970340</td>\n",
       "      <td>20140703T000000</td>\n",
       "      <td>285000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2270</td>\n",
       "      <td>6300</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2270</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>98092</td>\n",
       "      <td>47.3266</td>\n",
       "      <td>-122.169</td>\n",
       "      <td>2240</td>\n",
       "      <td>7005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8091400200</td>\n",
       "      <td>20140516T000000</td>\n",
       "      <td>252700.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1070</td>\n",
       "      <td>9643</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1070</td>\n",
       "      <td>0</td>\n",
       "      <td>1985</td>\n",
       "      <td>0</td>\n",
       "      <td>98030</td>\n",
       "      <td>47.3533</td>\n",
       "      <td>-122.166</td>\n",
       "      <td>1220</td>\n",
       "      <td>8386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3814700200</td>\n",
       "      <td>20141120T000000</td>\n",
       "      <td>329000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2450</td>\n",
       "      <td>6500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2450</td>\n",
       "      <td>0</td>\n",
       "      <td>1985</td>\n",
       "      <td>0</td>\n",
       "      <td>98030</td>\n",
       "      <td>47.3739</td>\n",
       "      <td>-122.172</td>\n",
       "      <td>2200</td>\n",
       "      <td>6865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1202000200</td>\n",
       "      <td>20141103T000000</td>\n",
       "      <td>233000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1710</td>\n",
       "      <td>4697</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1710</td>\n",
       "      <td>0</td>\n",
       "      <td>1941</td>\n",
       "      <td>0</td>\n",
       "      <td>98002</td>\n",
       "      <td>47.3048</td>\n",
       "      <td>-122.218</td>\n",
       "      <td>1030</td>\n",
       "      <td>4705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1794500383</td>\n",
       "      <td>20140626T000000</td>\n",
       "      <td>937000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2450</td>\n",
       "      <td>2691</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1750</td>\n",
       "      <td>700</td>\n",
       "      <td>1915</td>\n",
       "      <td>0</td>\n",
       "      <td>98119</td>\n",
       "      <td>47.6386</td>\n",
       "      <td>-122.360</td>\n",
       "      <td>1760</td>\n",
       "      <td>3573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3303700376</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>667000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1400</td>\n",
       "      <td>1581</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1400</td>\n",
       "      <td>0</td>\n",
       "      <td>1909</td>\n",
       "      <td>0</td>\n",
       "      <td>98112</td>\n",
       "      <td>47.6221</td>\n",
       "      <td>-122.314</td>\n",
       "      <td>1860</td>\n",
       "      <td>3861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5101402488</td>\n",
       "      <td>20140624T000000</td>\n",
       "      <td>438000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1520</td>\n",
       "      <td>6380</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>790</td>\n",
       "      <td>730</td>\n",
       "      <td>1948</td>\n",
       "      <td>0</td>\n",
       "      <td>98115</td>\n",
       "      <td>47.6950</td>\n",
       "      <td>-122.304</td>\n",
       "      <td>1520</td>\n",
       "      <td>6235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1873100390</td>\n",
       "      <td>20150302T000000</td>\n",
       "      <td>719000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2570</td>\n",
       "      <td>7173</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2570</td>\n",
       "      <td>0</td>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>98052</td>\n",
       "      <td>47.7073</td>\n",
       "      <td>-122.110</td>\n",
       "      <td>2630</td>\n",
       "      <td>6026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21583</th>\n",
       "      <td>2025049203</td>\n",
       "      <td>20140610T000000</td>\n",
       "      <td>399950.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>710</td>\n",
       "      <td>1157</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>710</td>\n",
       "      <td>0</td>\n",
       "      <td>1943</td>\n",
       "      <td>0</td>\n",
       "      <td>98102</td>\n",
       "      <td>47.6413</td>\n",
       "      <td>-122.329</td>\n",
       "      <td>1370</td>\n",
       "      <td>1173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21584</th>\n",
       "      <td>952006823</td>\n",
       "      <td>20141202T000000</td>\n",
       "      <td>380000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1260</td>\n",
       "      <td>900</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>940</td>\n",
       "      <td>320</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98116</td>\n",
       "      <td>47.5621</td>\n",
       "      <td>-122.384</td>\n",
       "      <td>1310</td>\n",
       "      <td>1415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21585</th>\n",
       "      <td>3832050760</td>\n",
       "      <td>20140828T000000</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1870</td>\n",
       "      <td>5000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1870</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98042</td>\n",
       "      <td>47.3339</td>\n",
       "      <td>-122.055</td>\n",
       "      <td>2170</td>\n",
       "      <td>5399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21586</th>\n",
       "      <td>2767604724</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>505000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1430</td>\n",
       "      <td>1201</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1430</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98107</td>\n",
       "      <td>47.6707</td>\n",
       "      <td>-122.381</td>\n",
       "      <td>1430</td>\n",
       "      <td>1249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21587</th>\n",
       "      <td>6632300207</td>\n",
       "      <td>20150305T000000</td>\n",
       "      <td>385000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1520</td>\n",
       "      <td>1488</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1520</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7337</td>\n",
       "      <td>-122.309</td>\n",
       "      <td>1520</td>\n",
       "      <td>1497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21588</th>\n",
       "      <td>2767600688</td>\n",
       "      <td>20141113T000000</td>\n",
       "      <td>414500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1210</td>\n",
       "      <td>1278</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1020</td>\n",
       "      <td>190</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98117</td>\n",
       "      <td>47.6756</td>\n",
       "      <td>-122.375</td>\n",
       "      <td>1210</td>\n",
       "      <td>1118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21589</th>\n",
       "      <td>7570050450</td>\n",
       "      <td>20140910T000000</td>\n",
       "      <td>347500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2540</td>\n",
       "      <td>4760</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2540</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.3452</td>\n",
       "      <td>-122.022</td>\n",
       "      <td>2540</td>\n",
       "      <td>4571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21590</th>\n",
       "      <td>7430200100</td>\n",
       "      <td>20140514T000000</td>\n",
       "      <td>1222500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4910</td>\n",
       "      <td>9444</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3110</td>\n",
       "      <td>1800</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6502</td>\n",
       "      <td>-122.066</td>\n",
       "      <td>4560</td>\n",
       "      <td>11063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21591</th>\n",
       "      <td>4140940150</td>\n",
       "      <td>20141002T000000</td>\n",
       "      <td>572000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2770</td>\n",
       "      <td>3852</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2770</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5001</td>\n",
       "      <td>-122.232</td>\n",
       "      <td>1810</td>\n",
       "      <td>5641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21592</th>\n",
       "      <td>1931300412</td>\n",
       "      <td>20150416T000000</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1190</td>\n",
       "      <td>1200</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1190</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6542</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1180</td>\n",
       "      <td>1224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21593</th>\n",
       "      <td>8672200110</td>\n",
       "      <td>20150317T000000</td>\n",
       "      <td>1088000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4170</td>\n",
       "      <td>8142</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>4170</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98056</td>\n",
       "      <td>47.5354</td>\n",
       "      <td>-122.181</td>\n",
       "      <td>3030</td>\n",
       "      <td>7980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21594</th>\n",
       "      <td>5087900040</td>\n",
       "      <td>20141017T000000</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2500</td>\n",
       "      <td>5995</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98042</td>\n",
       "      <td>47.3749</td>\n",
       "      <td>-122.107</td>\n",
       "      <td>2530</td>\n",
       "      <td>5988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21595</th>\n",
       "      <td>1972201967</td>\n",
       "      <td>20141031T000000</td>\n",
       "      <td>520000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1530</td>\n",
       "      <td>981</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1480</td>\n",
       "      <td>50</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6533</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21596</th>\n",
       "      <td>7502800100</td>\n",
       "      <td>20140813T000000</td>\n",
       "      <td>679950.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3600</td>\n",
       "      <td>9437</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3600</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98059</td>\n",
       "      <td>47.4822</td>\n",
       "      <td>-122.131</td>\n",
       "      <td>3550</td>\n",
       "      <td>9421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21597</th>\n",
       "      <td>191100405</td>\n",
       "      <td>20150421T000000</td>\n",
       "      <td>1575000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3410</td>\n",
       "      <td>10125</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>3410</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98040</td>\n",
       "      <td>47.5653</td>\n",
       "      <td>-122.223</td>\n",
       "      <td>2290</td>\n",
       "      <td>10125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21598</th>\n",
       "      <td>8956200760</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>541800.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3118</td>\n",
       "      <td>7866</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3118</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98001</td>\n",
       "      <td>47.2931</td>\n",
       "      <td>-122.264</td>\n",
       "      <td>2673</td>\n",
       "      <td>6500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21599</th>\n",
       "      <td>7202300110</td>\n",
       "      <td>20140915T000000</td>\n",
       "      <td>810000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3990</td>\n",
       "      <td>7838</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3990</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98053</td>\n",
       "      <td>47.6857</td>\n",
       "      <td>-122.046</td>\n",
       "      <td>3370</td>\n",
       "      <td>6814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21600</th>\n",
       "      <td>249000205</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>1537000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4470</td>\n",
       "      <td>8088</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>4470</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98004</td>\n",
       "      <td>47.6321</td>\n",
       "      <td>-122.200</td>\n",
       "      <td>2780</td>\n",
       "      <td>8964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21601</th>\n",
       "      <td>5100403806</td>\n",
       "      <td>20150407T000000</td>\n",
       "      <td>467000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1425</td>\n",
       "      <td>1179</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1425</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.6963</td>\n",
       "      <td>-122.318</td>\n",
       "      <td>1285</td>\n",
       "      <td>1253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21602</th>\n",
       "      <td>844000965</td>\n",
       "      <td>20140626T000000</td>\n",
       "      <td>224000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1500</td>\n",
       "      <td>11968</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98010</td>\n",
       "      <td>47.3095</td>\n",
       "      <td>-122.002</td>\n",
       "      <td>1320</td>\n",
       "      <td>11303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21603</th>\n",
       "      <td>7852140040</td>\n",
       "      <td>20140825T000000</td>\n",
       "      <td>507250.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2270</td>\n",
       "      <td>5536</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2270</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98065</td>\n",
       "      <td>47.5389</td>\n",
       "      <td>-121.881</td>\n",
       "      <td>2270</td>\n",
       "      <td>5731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21604</th>\n",
       "      <td>9834201367</td>\n",
       "      <td>20150126T000000</td>\n",
       "      <td>429000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1490</td>\n",
       "      <td>1126</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1490</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5699</td>\n",
       "      <td>-122.288</td>\n",
       "      <td>1400</td>\n",
       "      <td>1230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21605</th>\n",
       "      <td>3448900210</td>\n",
       "      <td>20141014T000000</td>\n",
       "      <td>610685.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2520</td>\n",
       "      <td>6023</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2520</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98056</td>\n",
       "      <td>47.5137</td>\n",
       "      <td>-122.167</td>\n",
       "      <td>2520</td>\n",
       "      <td>6023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21606</th>\n",
       "      <td>7936000429</td>\n",
       "      <td>20150326T000000</td>\n",
       "      <td>1007500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3510</td>\n",
       "      <td>7200</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2600</td>\n",
       "      <td>910</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5537</td>\n",
       "      <td>-122.398</td>\n",
       "      <td>2050</td>\n",
       "      <td>6200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21607</th>\n",
       "      <td>2997800021</td>\n",
       "      <td>20150219T000000</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1310</td>\n",
       "      <td>1294</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1180</td>\n",
       "      <td>130</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98116</td>\n",
       "      <td>47.5773</td>\n",
       "      <td>-122.409</td>\n",
       "      <td>1330</td>\n",
       "      <td>1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21608</th>\n",
       "      <td>263000018</td>\n",
       "      <td>20140521T000000</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1530</td>\n",
       "      <td>1131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1530</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6993</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21609</th>\n",
       "      <td>6600060120</td>\n",
       "      <td>20150223T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2310</td>\n",
       "      <td>5813</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2310</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5107</td>\n",
       "      <td>-122.362</td>\n",
       "      <td>1830</td>\n",
       "      <td>7200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21610</th>\n",
       "      <td>1523300141</td>\n",
       "      <td>20140623T000000</td>\n",
       "      <td>402101.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5944</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21611</th>\n",
       "      <td>291310100</td>\n",
       "      <td>20150116T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1600</td>\n",
       "      <td>2388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>98027</td>\n",
       "      <td>47.5345</td>\n",
       "      <td>-122.069</td>\n",
       "      <td>1410</td>\n",
       "      <td>1287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21612</th>\n",
       "      <td>1523300157</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5941</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21613 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id             date      price  bedrooms  bathrooms  \\\n",
       "0      7129300520  20141013T000000   221900.0         3       1.00   \n",
       "1      6414100192  20141209T000000   538000.0         3       2.25   \n",
       "2      5631500400  20150225T000000   180000.0         2       1.00   \n",
       "3      2487200875  20141209T000000   604000.0         4       3.00   \n",
       "4      1954400510  20150218T000000   510000.0         3       2.00   \n",
       "5      7237550310  20140512T000000  1225000.0         4       4.50   \n",
       "6      1321400060  20140627T000000   257500.0         3       2.25   \n",
       "7      2008000270  20150115T000000   291850.0         3       1.50   \n",
       "8      2414600126  20150415T000000   229500.0         3       1.00   \n",
       "9      3793500160  20150312T000000   323000.0         3       2.50   \n",
       "10     1736800520  20150403T000000   662500.0         3       2.50   \n",
       "11     9212900260  20140527T000000   468000.0         2       1.00   \n",
       "12      114101516  20140528T000000   310000.0         3       1.00   \n",
       "13     6054650070  20141007T000000   400000.0         3       1.75   \n",
       "14     1175000570  20150312T000000   530000.0         5       2.00   \n",
       "15     9297300055  20150124T000000   650000.0         4       3.00   \n",
       "16     1875500060  20140731T000000   395000.0         3       2.00   \n",
       "17     6865200140  20140529T000000   485000.0         4       1.00   \n",
       "18       16000397  20141205T000000   189000.0         2       1.00   \n",
       "19     7983200060  20150424T000000   230000.0         3       1.00   \n",
       "20     6300500875  20140514T000000   385000.0         4       1.75   \n",
       "21     2524049179  20140826T000000  2000000.0         3       2.75   \n",
       "22     7137970340  20140703T000000   285000.0         5       2.50   \n",
       "23     8091400200  20140516T000000   252700.0         2       1.50   \n",
       "24     3814700200  20141120T000000   329000.0         3       2.25   \n",
       "25     1202000200  20141103T000000   233000.0         3       2.00   \n",
       "26     1794500383  20140626T000000   937000.0         3       1.75   \n",
       "27     3303700376  20141201T000000   667000.0         3       1.00   \n",
       "28     5101402488  20140624T000000   438000.0         3       1.75   \n",
       "29     1873100390  20150302T000000   719000.0         4       2.50   \n",
       "...           ...              ...        ...       ...        ...   \n",
       "21583  2025049203  20140610T000000   399950.0         2       1.00   \n",
       "21584   952006823  20141202T000000   380000.0         3       2.50   \n",
       "21585  3832050760  20140828T000000   270000.0         3       2.50   \n",
       "21586  2767604724  20141015T000000   505000.0         2       2.50   \n",
       "21587  6632300207  20150305T000000   385000.0         3       2.50   \n",
       "21588  2767600688  20141113T000000   414500.0         2       1.50   \n",
       "21589  7570050450  20140910T000000   347500.0         3       2.50   \n",
       "21590  7430200100  20140514T000000  1222500.0         4       3.50   \n",
       "21591  4140940150  20141002T000000   572000.0         4       2.75   \n",
       "21592  1931300412  20150416T000000   475000.0         3       2.25   \n",
       "21593  8672200110  20150317T000000  1088000.0         5       3.75   \n",
       "21594  5087900040  20141017T000000   350000.0         4       2.75   \n",
       "21595  1972201967  20141031T000000   520000.0         2       2.25   \n",
       "21596  7502800100  20140813T000000   679950.0         5       2.75   \n",
       "21597   191100405  20150421T000000  1575000.0         4       3.25   \n",
       "21598  8956200760  20141013T000000   541800.0         4       2.50   \n",
       "21599  7202300110  20140915T000000   810000.0         4       3.00   \n",
       "21600   249000205  20141015T000000  1537000.0         5       3.75   \n",
       "21601  5100403806  20150407T000000   467000.0         3       2.50   \n",
       "21602   844000965  20140626T000000   224000.0         3       1.75   \n",
       "21603  7852140040  20140825T000000   507250.0         3       2.50   \n",
       "21604  9834201367  20150126T000000   429000.0         3       2.00   \n",
       "21605  3448900210  20141014T000000   610685.0         4       2.50   \n",
       "21606  7936000429  20150326T000000  1007500.0         4       3.50   \n",
       "21607  2997800021  20150219T000000   475000.0         3       2.50   \n",
       "21608   263000018  20140521T000000   360000.0         3       2.50   \n",
       "21609  6600060120  20150223T000000   400000.0         4       2.50   \n",
       "21610  1523300141  20140623T000000   402101.0         2       0.75   \n",
       "21611   291310100  20150116T000000   400000.0         3       2.50   \n",
       "21612  1523300157  20141015T000000   325000.0         2       0.75   \n",
       "\n",
       "       sqft_living  sqft_lot  floors  waterfront  view  ...  grade  \\\n",
       "0             1180      5650     1.0           0     0  ...      7   \n",
       "1             2570      7242     2.0           0     0  ...      7   \n",
       "2              770     10000     1.0           0     0  ...      6   \n",
       "3             1960      5000     1.0           0     0  ...      7   \n",
       "4             1680      8080     1.0           0     0  ...      8   \n",
       "5             5420    101930     1.0           0     0  ...     11   \n",
       "6             1715      6819     2.0           0     0  ...      7   \n",
       "7             1060      9711     1.0           0     0  ...      7   \n",
       "8             1780      7470     1.0           0     0  ...      7   \n",
       "9             1890      6560     2.0           0     0  ...      7   \n",
       "10            3560      9796     1.0           0     0  ...      8   \n",
       "11            1160      6000     1.0           0     0  ...      7   \n",
       "12            1430     19901     1.5           0     0  ...      7   \n",
       "13            1370      9680     1.0           0     0  ...      7   \n",
       "14            1810      4850     1.5           0     0  ...      7   \n",
       "15            2950      5000     2.0           0     3  ...      9   \n",
       "16            1890     14040     2.0           0     0  ...      7   \n",
       "17            1600      4300     1.5           0     0  ...      7   \n",
       "18            1200      9850     1.0           0     0  ...      7   \n",
       "19            1250      9774     1.0           0     0  ...      7   \n",
       "20            1620      4980     1.0           0     0  ...      7   \n",
       "21            3050     44867     1.0           0     4  ...      9   \n",
       "22            2270      6300     2.0           0     0  ...      8   \n",
       "23            1070      9643     1.0           0     0  ...      7   \n",
       "24            2450      6500     2.0           0     0  ...      8   \n",
       "25            1710      4697     1.5           0     0  ...      6   \n",
       "26            2450      2691     2.0           0     0  ...      8   \n",
       "27            1400      1581     1.5           0     0  ...      8   \n",
       "28            1520      6380     1.0           0     0  ...      7   \n",
       "29            2570      7173     2.0           0     0  ...      8   \n",
       "...            ...       ...     ...         ...   ...  ...    ...   \n",
       "21583          710      1157     2.0           0     0  ...      7   \n",
       "21584         1260       900     2.0           0     0  ...      7   \n",
       "21585         1870      5000     2.0           0     0  ...      7   \n",
       "21586         1430      1201     3.0           0     0  ...      8   \n",
       "21587         1520      1488     3.0           0     0  ...      8   \n",
       "21588         1210      1278     2.0           0     0  ...      8   \n",
       "21589         2540      4760     2.0           0     0  ...      8   \n",
       "21590         4910      9444     1.5           0     0  ...     11   \n",
       "21591         2770      3852     2.0           0     0  ...      8   \n",
       "21592         1190      1200     3.0           0     0  ...      8   \n",
       "21593         4170      8142     2.0           0     2  ...     10   \n",
       "21594         2500      5995     2.0           0     0  ...      8   \n",
       "21595         1530       981     3.0           0     0  ...      8   \n",
       "21596         3600      9437     2.0           0     0  ...      9   \n",
       "21597         3410     10125     2.0           0     0  ...     10   \n",
       "21598         3118      7866     2.0           0     2  ...      9   \n",
       "21599         3990      7838     2.0           0     0  ...      9   \n",
       "21600         4470      8088     2.0           0     0  ...     11   \n",
       "21601         1425      1179     3.0           0     0  ...      8   \n",
       "21602         1500     11968     1.0           0     0  ...      6   \n",
       "21603         2270      5536     2.0           0     0  ...      8   \n",
       "21604         1490      1126     3.0           0     0  ...      8   \n",
       "21605         2520      6023     2.0           0     0  ...      9   \n",
       "21606         3510      7200     2.0           0     0  ...      9   \n",
       "21607         1310      1294     2.0           0     0  ...      8   \n",
       "21608         1530      1131     3.0           0     0  ...      8   \n",
       "21609         2310      5813     2.0           0     0  ...      8   \n",
       "21610         1020      1350     2.0           0     0  ...      7   \n",
       "21611         1600      2388     2.0           0     0  ...      8   \n",
       "21612         1020      1076     2.0           0     0  ...      7   \n",
       "\n",
       "       sqft_above  sqft_basement  yr_built  yr_renovated  zipcode      lat  \\\n",
       "0            1180              0      1955             0    98178  47.5112   \n",
       "1            2170            400      1951          1991    98125  47.7210   \n",
       "2             770              0      1933             0    98028  47.7379   \n",
       "3            1050            910      1965             0    98136  47.5208   \n",
       "4            1680              0      1987             0    98074  47.6168   \n",
       "5            3890           1530      2001             0    98053  47.6561   \n",
       "6            1715              0      1995             0    98003  47.3097   \n",
       "7            1060              0      1963             0    98198  47.4095   \n",
       "8            1050            730      1960             0    98146  47.5123   \n",
       "9            1890              0      2003             0    98038  47.3684   \n",
       "10           1860           1700      1965             0    98007  47.6007   \n",
       "11            860            300      1942             0    98115  47.6900   \n",
       "12           1430              0      1927             0    98028  47.7558   \n",
       "13           1370              0      1977             0    98074  47.6127   \n",
       "14           1810              0      1900             0    98107  47.6700   \n",
       "15           1980            970      1979             0    98126  47.5714   \n",
       "16           1890              0      1994             0    98019  47.7277   \n",
       "17           1600              0      1916             0    98103  47.6648   \n",
       "18           1200              0      1921             0    98002  47.3089   \n",
       "19           1250              0      1969             0    98003  47.3343   \n",
       "20            860            760      1947             0    98133  47.7025   \n",
       "21           2330            720      1968             0    98040  47.5316   \n",
       "22           2270              0      1995             0    98092  47.3266   \n",
       "23           1070              0      1985             0    98030  47.3533   \n",
       "24           2450              0      1985             0    98030  47.3739   \n",
       "25           1710              0      1941             0    98002  47.3048   \n",
       "26           1750            700      1915             0    98119  47.6386   \n",
       "27           1400              0      1909             0    98112  47.6221   \n",
       "28            790            730      1948             0    98115  47.6950   \n",
       "29           2570              0      2005             0    98052  47.7073   \n",
       "...           ...            ...       ...           ...      ...      ...   \n",
       "21583         710              0      1943             0    98102  47.6413   \n",
       "21584         940            320      2007             0    98116  47.5621   \n",
       "21585        1870              0      2009             0    98042  47.3339   \n",
       "21586        1430              0      2009             0    98107  47.6707   \n",
       "21587        1520              0      2006             0    98125  47.7337   \n",
       "21588        1020            190      2007             0    98117  47.6756   \n",
       "21589        2540              0      2010             0    98038  47.3452   \n",
       "21590        3110           1800      2007             0    98074  47.6502   \n",
       "21591        2770              0      2014             0    98178  47.5001   \n",
       "21592        1190              0      2008             0    98103  47.6542   \n",
       "21593        4170              0      2006             0    98056  47.5354   \n",
       "21594        2500              0      2008             0    98042  47.3749   \n",
       "21595        1480             50      2006             0    98103  47.6533   \n",
       "21596        3600              0      2014             0    98059  47.4822   \n",
       "21597        3410              0      2007             0    98040  47.5653   \n",
       "21598        3118              0      2014             0    98001  47.2931   \n",
       "21599        3990              0      2003             0    98053  47.6857   \n",
       "21600        4470              0      2008             0    98004  47.6321   \n",
       "21601        1425              0      2008             0    98125  47.6963   \n",
       "21602        1500              0      2014             0    98010  47.3095   \n",
       "21603        2270              0      2003             0    98065  47.5389   \n",
       "21604        1490              0      2014             0    98144  47.5699   \n",
       "21605        2520              0      2014             0    98056  47.5137   \n",
       "21606        2600            910      2009             0    98136  47.5537   \n",
       "21607        1180            130      2008             0    98116  47.5773   \n",
       "21608        1530              0      2009             0    98103  47.6993   \n",
       "21609        2310              0      2014             0    98146  47.5107   \n",
       "21610        1020              0      2009             0    98144  47.5944   \n",
       "21611        1600              0      2004             0    98027  47.5345   \n",
       "21612        1020              0      2008             0    98144  47.5941   \n",
       "\n",
       "          long  sqft_living15  sqft_lot15  \n",
       "0     -122.257           1340        5650  \n",
       "1     -122.319           1690        7639  \n",
       "2     -122.233           2720        8062  \n",
       "3     -122.393           1360        5000  \n",
       "4     -122.045           1800        7503  \n",
       "5     -122.005           4760      101930  \n",
       "6     -122.327           2238        6819  \n",
       "7     -122.315           1650        9711  \n",
       "8     -122.337           1780        8113  \n",
       "9     -122.031           2390        7570  \n",
       "10    -122.145           2210        8925  \n",
       "11    -122.292           1330        6000  \n",
       "12    -122.229           1780       12697  \n",
       "13    -122.045           1370       10208  \n",
       "14    -122.394           1360        4850  \n",
       "15    -122.375           2140        4000  \n",
       "16    -121.962           1890       14018  \n",
       "17    -122.343           1610        4300  \n",
       "18    -122.210           1060        5095  \n",
       "19    -122.306           1280        8850  \n",
       "20    -122.341           1400        4980  \n",
       "21    -122.233           4110       20336  \n",
       "22    -122.169           2240        7005  \n",
       "23    -122.166           1220        8386  \n",
       "24    -122.172           2200        6865  \n",
       "25    -122.218           1030        4705  \n",
       "26    -122.360           1760        3573  \n",
       "27    -122.314           1860        3861  \n",
       "28    -122.304           1520        6235  \n",
       "29    -122.110           2630        6026  \n",
       "...        ...            ...         ...  \n",
       "21583 -122.329           1370        1173  \n",
       "21584 -122.384           1310        1415  \n",
       "21585 -122.055           2170        5399  \n",
       "21586 -122.381           1430        1249  \n",
       "21587 -122.309           1520        1497  \n",
       "21588 -122.375           1210        1118  \n",
       "21589 -122.022           2540        4571  \n",
       "21590 -122.066           4560       11063  \n",
       "21591 -122.232           1810        5641  \n",
       "21592 -122.346           1180        1224  \n",
       "21593 -122.181           3030        7980  \n",
       "21594 -122.107           2530        5988  \n",
       "21595 -122.346           1530        1282  \n",
       "21596 -122.131           3550        9421  \n",
       "21597 -122.223           2290       10125  \n",
       "21598 -122.264           2673        6500  \n",
       "21599 -122.046           3370        6814  \n",
       "21600 -122.200           2780        8964  \n",
       "21601 -122.318           1285        1253  \n",
       "21602 -122.002           1320       11303  \n",
       "21603 -121.881           2270        5731  \n",
       "21604 -122.288           1400        1230  \n",
       "21605 -122.167           2520        6023  \n",
       "21606 -122.398           2050        6200  \n",
       "21607 -122.409           1330        1265  \n",
       "21608 -122.346           1530        1509  \n",
       "21609 -122.362           1830        7200  \n",
       "21610 -122.299           1020        2007  \n",
       "21611 -122.069           1410        1287  \n",
       "21612 -122.299           1020        1357  \n",
       "\n",
       "[21613 rows x 21 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "path=\"KC House Data/kc_house_data.csv\"\n",
    "datos_df=pd.read_csv(path) \n",
    "datos_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A continuación se listan las variables incluídas en la base de datos. Selecciona una lista de variables para utilizar en la regresión (como se muestra en la siguiente celda) y argumenta porqué harías esa selección. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'date', 'price', 'bedrooms', 'bathrooms', 'sqft_living',\n",
       "       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n",
       "       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n",
       "       'lat', 'long', 'sqft_living15', 'sqft_lot15'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables=['id', 'date', 'price', 'bedrooms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables=['price', 'bedrooms', 'bathrooms', 'sqft_living',\n",
    "       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n",
    "       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'sqft_living15', 'sqft_lot15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "corr_mat=datos_df[variables].corr(method='pearson')\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(corr_mat,vmax=1,square=True,annot=True,cmap='cubehelix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Separa las variables en independientes (X) y dependientes (y). Posteriormente, haz una separación entre prueba y el resto de los datos (validación y entrenamiento) utilizando `train_test_split`. Separa 20% de los datos para prueba.\n",
    "\n",
    "Nota: Al igual que en el problema 1 puedes seleccionar una submuestra de datos para utilizar durante el resto del problema si tu máquina no tiene mucha capacidad de procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_data=datos_df[variables[1:]]\n",
    "y_labels=datos_df[variables[0]]\n",
    "\n",
    "X_rest, X_test, y_rest, y_test = train_test_split(X_data,y_labels,test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Ahora, con las variables seleccionadas haremos un ejercicio de regresión. De manera análoga al inciso 5 del problema 1, utiliza la función `GridSearchCV()` con las funciones y los diccionarios que se presentan en la siguiente celda para crear un modelo final para cada algoritmo. \n",
    "\n",
    "    Los algoritmos que utilizaremos son: regresión lineal con OLS, Ridge, LASSO y Elastic Net, regresión con árboles de decisión y bosques aleatorios y regresión con SVMs.\n",
    "    La métrica que utilizaremos es el error cuadrático medio. Durante el entrenamiento puedes utilizar como scoring parámetro `neg_mean_squared_error` para obtener el negativo de ese valor y, durante la evaluación con los datos de prueba puedes utilizar la función `mean_squared_error` que se incluye a continuación.[[link]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "dtree_grid={'criterion':['mse','mae'],'max_depth':[2,5,7,10,15,20],\n",
    "           'min_samples_split':[2,4,7,10,15]}\n",
    "forest_grid={'n_estimators':[2,5,10,40,75],'criterion':['mse','mae'],\n",
    "             'max_depth':[2,5,7,10,15,20],'min_samples_split':[2,4,7,10,15]}\n",
    "\n",
    "svr_grid={\"C\":[0.01,0.1,1,10,100,1000,10000],'epsilon':[0.01,0.1,1],\n",
    "          'kernel':['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "\n",
    "ols_grid = {'normalize':[True,False]}\n",
    "ridge_grid = {'normalize':[True,False],'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n",
    "lasso_grid = {'normalize':[True,False],'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n",
    "enet_grid = {'normalize':[True,False],'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20],'l1_ratio':[0.1,0.25,0.5,0.75,0.9]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  a) Para cada algoritmo, haz una búsqueda en la malla de hiperparámetros, entrenando con validación cruzada de 5 cortes y selecciona el modelo con el menor error cuadrático medio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=6)]: Done 266 tasks      | elapsed:  3.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-46248990246.611534\n",
      "{'criterion': 'mse', 'max_depth': 7, 'min_samples_split': 15}\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done 300 out of 300 | elapsed:  4.1min finished\n"
     ]
    }
   ],
   "source": [
    "dtree_cv=GridSearchCV(DecisionTreeRegressor(), \n",
    "                      dtree_grid, scoring='neg_mean_squared_error', n_jobs=6, iid=False, refit=True, cv=5, \n",
    "                      verbose=2, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=True)\n",
    "dtree_cv.fit(X_rest,y_rest)\n",
    "print(dtree_cv.best_score_)\n",
    "print(dtree_cv.best_params_)\n",
    "print(dtree_cv.best_index_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done 108 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=6)]: Done 259 tasks      | elapsed:   23.6s\n",
      "[Parallel(n_jobs=6)]: Done 462 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=6)]: Done 745 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=6)]: Done 1110 tasks      | elapsed: 96.0min\n",
      "[Parallel(n_jobs=6)]: Done 1500 out of 1500 | elapsed: 229.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-34582837940.40892\n",
      "{'criterion': 'mse', 'max_depth': 20, 'min_samples_split': 4, 'n_estimators': 75}\n",
      "134\n"
     ]
    }
   ],
   "source": [
    "forest_cv=GridSearchCV(RandomForestRegressor(), \n",
    "                      forest_grid, scoring='neg_mean_squared_error', n_jobs=6, iid=False, refit=True, cv=5, \n",
    "                      verbose=2, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=True)\n",
    "forest_cv.fit(X_rest,y_rest)\n",
    "print(forest_cv.best_score_)\n",
    "print(forest_cv.best_params_)\n",
    "print(forest_cv.best_index_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 84 candidates, totalling 420 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=6)]: Done 150 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=6)]: Done 353 tasks      | elapsed: 16.1min\n",
      "[Parallel(n_jobs=6)]: Done 420 out of 420 | elapsed: 18.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-80769414727.41122\n",
      "{'C': 0.01, 'epsilon': 0.1, 'kernel': 'linear'}\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "svr_cv=GridSearchCV(SVR(max_iter=50000), \n",
    "                      svr_grid, scoring='neg_mean_squared_error', n_jobs=6, iid=False, refit=True, cv=5, \n",
    "                      verbose=2, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=True)\n",
    "svr_cv.fit(X_rest,y_rest)\n",
    "print(svr_cv.best_score_)\n",
    "print(svr_cv.best_params_)\n",
    "print(svr_cv.best_index_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "-48256614168.73189\n",
      "{'normalize': False}\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   5 out of  10 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "ols_cv=GridSearchCV(LinearRegression(), \n",
    "                      ols_grid, scoring='neg_mean_squared_error', n_jobs=6, iid=False, refit=True, cv=5, \n",
    "                      verbose=2, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=True)\n",
    "ols_cv.fit(X_rest,y_rest)\n",
    "print(ols_cv.best_score_)\n",
    "print(ols_cv.best_params_)\n",
    "print(ols_cv.best_index_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-48255978151.92046\n",
      "{'alpha': 0.001, 'normalize': True}\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done 100 out of 100 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "ridge_cv=GridSearchCV(Ridge(), \n",
    "                      ridge_grid, scoring='neg_mean_squared_error', n_jobs=6, iid=False, refit=True, cv=5, \n",
    "                      verbose=2, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=True)\n",
    "ridge_cv.fit(X_rest,y_rest)\n",
    "print(ridge_cv.best_score_)\n",
    "print(ridge_cv.best_params_)\n",
    "print(ridge_cv.best_index_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:    3.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-48250816280.08183\n",
      "{'alpha': 5, 'normalize': True}\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done 100 out of 100 | elapsed:    9.4s finished\n"
     ]
    }
   ],
   "source": [
    "lasso_cv=GridSearchCV(Lasso(), \n",
    "                      lasso_grid, scoring='neg_mean_squared_error', n_jobs=6, iid=False, refit=True, cv=5, \n",
    "                      verbose=2, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=True)\n",
    "lasso_cv.fit(X_rest,y_rest)\n",
    "print(lasso_cv.best_score_)\n",
    "print(lasso_cv.best_params_)\n",
    "print(lasso_cv.best_index_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=6)]: Done 150 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=6)]: Done 353 tasks      | elapsed:   29.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-48256363307.87817\n",
      "{'alpha': 0.0001, 'l1_ratio': 0.25, 'normalize': False}\n",
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done 500 out of 500 | elapsed:   37.9s finished\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "enet_cv=GridSearchCV(ElasticNet(), \n",
    "                      enet_grid, scoring='neg_mean_squared_error', n_jobs=6, iid=False, refit=True, cv=5, \n",
    "                      verbose=2, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=True)\n",
    "enet_cv.fit(X_rest,y_rest)\n",
    "print(enet_cv.best_score_)\n",
    "print(enet_cv.best_params_)\n",
    "print(enet_cv.best_index_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. b)Calcula el error cuadrático medio sobre el conjunto de prueba, para los modelos seleccionados en el inciso anterior. ¿El error resultó se mayor o menor? ¿A qué se debe esto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38789749145.83796"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_test=dtree_cv.best_estimator_.predict(X_test)\n",
    "mse_dtree=mean_squared_error(y_test, predicted_test)\n",
    "mse_dtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29713545785.906986"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_test=forest_cv.best_estimator_.predict(X_test)\n",
    "mse_forest=mean_squared_error(y_test, predicted_test)\n",
    "mse_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73747236754.41423"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_test=svr_cv.best_estimator_.predict(X_test)\n",
    "mse_svr=mean_squared_error(y_test, predicted_test)\n",
    "mse_svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41601961823.16162"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_test=ols_cv.best_estimator_.predict(X_test)\n",
    "mse_ols=mean_squared_error(y_test, predicted_test)\n",
    "mse_ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41602234482.24971"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_test=ridge_cv.best_estimator_.predict(X_test)\n",
    "mse_ridge=mean_squared_error(y_test, predicted_test)\n",
    "mse_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41642636236.81948"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_test=lasso_cv.best_estimator_.predict(X_test)\n",
    "mse_lasso=mean_squared_error(y_test, predicted_test)\n",
    "mse_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41623202263.75211"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_test=enet_cv.best_estimator_.predict(X_test)\n",
    "mse_enet=mean_squared_error(y_test, predicted_test)\n",
    "mse_enet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38789749145.83796\n",
      "29713545785.906986\n",
      "73747236754.41423\n",
      "41601961823.16162\n",
      "41602234482.24971\n",
      "41642636236.81948\n",
      "41623202263.75211\n"
     ]
    }
   ],
   "source": [
    "print(mse_dtree)\n",
    "print(mse_forest)\n",
    "print(mse_svr)\n",
    "print(mse_ols)\n",
    "print(mse_ridge)\n",
    "print(mse_lasso)\n",
    "print(mse_enet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version Normalizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_data=datos_df[variables[1:]]\n",
    "X_data=(X_data-X_data.mean())/X_data.std()\n",
    "y_labels=datos_df[variables[0]]\n",
    "\n",
    "X_rest, X_test, y_rest, y_test = train_test_split(X_data,y_labels,test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:    0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-48898630045.51626\n",
      "{'criterion': 'mse', 'max_depth': 7, 'min_samples_split': 15}\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done 300 out of 300 | elapsed:  3.9min finished\n"
     ]
    }
   ],
   "source": [
    "dtree_cv=GridSearchCV(DecisionTreeRegressor(), \n",
    "                      dtree_grid, scoring='neg_mean_squared_error', n_jobs=6, iid=False, refit=True, cv=5, \n",
    "                      verbose=2, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=True)\n",
    "dtree_cv.fit(X_rest,y_rest)\n",
    "print(dtree_cv.best_score_)\n",
    "print(dtree_cv.best_params_)\n",
    "print(dtree_cv.best_index_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  27 tasks      | elapsed:   28.0s\n",
      "[Parallel(n_jobs=7)]: Done 148 tasks      | elapsed:   35.1s\n",
      "[Parallel(n_jobs=7)]: Done 351 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=7)]: Done 634 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=7)]: Done 999 tasks      | elapsed: 59.5min\n",
      "[Parallel(n_jobs=7)]: Done 1444 tasks      | elapsed: 202.8min\n",
      "[Parallel(n_jobs=7)]: Done 1500 out of 1500 | elapsed: 225.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-36241573589.04747\n",
      "{'criterion': 'mse', 'max_depth': 20, 'min_samples_split': 7, 'n_estimators': 75}\n",
      "139\n"
     ]
    }
   ],
   "source": [
    "forest_cv=GridSearchCV(RandomForestRegressor(), \n",
    "                      forest_grid, scoring='neg_mean_squared_error', n_jobs=7, iid=False, refit=True, cv=5, \n",
    "                      verbose=2, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=True)\n",
    "forest_cv.fit(X_rest,y_rest)\n",
    "print(forest_cv.best_score_)\n",
    "print(forest_cv.best_params_)\n",
    "print(forest_cv.best_index_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 84 candidates, totalling 420 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=6)]: Done 150 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=6)]: Done 353 tasks      | elapsed: 21.9min\n",
      "[Parallel(n_jobs=6)]: Done 420 out of 420 | elapsed: 26.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-51192016649.03522\n",
      "{'C': 10000, 'epsilon': 0.01, 'kernel': 'linear'}\n",
      "72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "svr_cv=GridSearchCV(SVR(max_iter=50000), \n",
    "                      svr_grid, scoring='neg_mean_squared_error', n_jobs=6, iid=False, refit=True, cv=5, \n",
    "                      verbose=2, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=True)\n",
    "svr_cv.fit(X_rest,y_rest)\n",
    "print(svr_cv.best_score_)\n",
    "print(svr_cv.best_params_)\n",
    "print(svr_cv.best_index_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   5 out of  10 | elapsed:    0.1s remaining:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-47642536301.756424\n",
      "{'normalize': False}\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "ols_cv=GridSearchCV(LinearRegression(), \n",
    "                      ols_grid, scoring='neg_mean_squared_error', n_jobs=6, iid=False, refit=True, cv=5, \n",
    "                      verbose=2, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=True)\n",
    "ols_cv.fit(X_rest,y_rest)\n",
    "print(ols_cv.best_score_)\n",
    "print(ols_cv.best_params_)\n",
    "print(ols_cv.best_index_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "-47640856825.7829\n",
      "{'alpha': 0.01, 'normalize': True}\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done 100 out of 100 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "ridge_cv=GridSearchCV(Ridge(), \n",
    "                      ridge_grid, scoring='neg_mean_squared_error', n_jobs=6, iid=False, refit=True, cv=5, \n",
    "                      verbose=2, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=True)\n",
    "ridge_cv.fit(X_rest,y_rest)\n",
    "print(ridge_cv.best_score_)\n",
    "print(ridge_cv.best_params_)\n",
    "print(ridge_cv.best_index_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:    3.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-47642800696.60208\n",
      "{'alpha': 5, 'normalize': True}\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done 100 out of 100 | elapsed:    9.6s finished\n"
     ]
    }
   ],
   "source": [
    "lasso_cv=GridSearchCV(Lasso(), \n",
    "                      lasso_grid, scoring='neg_mean_squared_error', n_jobs=6, iid=False, refit=True, cv=5, \n",
    "                      verbose=2, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=True)\n",
    "lasso_cv.fit(X_rest,y_rest)\n",
    "print(lasso_cv.best_score_)\n",
    "print(lasso_cv.best_params_)\n",
    "print(lasso_cv.best_index_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=6)]: Done 150 tasks      | elapsed:   16.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-47640126728.30654\n",
      "{'alpha': 0.01, 'l1_ratio': 0.1, 'normalize': False}\n",
      "51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done 500 out of 500 | elapsed:   24.9s finished\n"
     ]
    }
   ],
   "source": [
    "enet_cv=GridSearchCV(ElasticNet(), \n",
    "                      enet_grid, scoring='neg_mean_squared_error', n_jobs=6, iid=False, refit=True, cv=5, \n",
    "                      verbose=2, pre_dispatch='2*n_jobs', error_score=np.nan, return_train_score=True)\n",
    "enet_cv.fit(X_rest,y_rest)\n",
    "print(enet_cv.best_score_)\n",
    "print(enet_cv.best_params_)\n",
    "print(enet_cv.best_index_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Fit MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48942999805.82949\n",
      "36342643600.833565\n",
      "48342824391.90656\n",
      "45478384458.57061\n",
      "45441151565.863304\n",
      "45453065140.32379\n",
      "45443345830.43461\n"
     ]
    }
   ],
   "source": [
    "predicted_test=dtree_cv.best_estimator_.predict(X_test)\n",
    "std_mse_dtree=mean_squared_error(y_test, predicted_test)\n",
    "print(std_mse_dtree)\n",
    "\n",
    "predicted_test=forest_cv.best_estimator_.predict(X_test)\n",
    "std_mse_forest=mean_squared_error(y_test, predicted_test)\n",
    "print(std_mse_forest)\n",
    "\n",
    "predicted_test=svr_cv.best_estimator_.predict(X_test)\n",
    "std_mse_svr=mean_squared_error(y_test, predicted_test)\n",
    "print(std_mse_svr)\n",
    "\n",
    "predicted_test=ols_cv.best_estimator_.predict(X_test)\n",
    "std_mse_ols=mean_squared_error(y_test, predicted_test)\n",
    "print(std_mse_ols)\n",
    "\n",
    "predicted_test=ridge_cv.best_estimator_.predict(X_test)\n",
    "std_mse_ridge=mean_squared_error(y_test, predicted_test)\n",
    "print(std_mse_ridge)\n",
    "\n",
    "predicted_test=lasso_cv.best_estimator_.predict(X_test)\n",
    "std_mse_lasso=mean_squared_error(y_test, predicted_test)\n",
    "print(std_mse_lasso)\n",
    "\n",
    "predicted_test=enet_cv.best_estimator_.predict(X_test)\n",
    "std_mse_enet=mean_squared_error(y_test, predicted_test)\n",
    "print(std_mse_enet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38789749145.83796\n",
      "29713545785.906986\n",
      "73747236754.41423\n",
      "41601961823.16162\n",
      "41602234482.24971\n",
      "41642636236.81948\n",
      "41623202263.75211\n"
     ]
    }
   ],
   "source": [
    "print(mse_dtree)\n",
    "print(mse_forest)\n",
    "print(mse_svr)\n",
    "print(mse_ols)\n",
    "print(mse_ridge)\n",
    "print(mse_lasso)\n",
    "print(mse_enet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10153250659.991531\n",
      "-6629097814.9265785\n",
      "25404412362.507668\n",
      "-3876422635.408989\n",
      "-3838917083.613594\n",
      "-3810428903.5043106\n",
      "-3820143566.682495\n"
     ]
    }
   ],
   "source": [
    "print(mse_dtree-std_mse_dtree)\n",
    "print(mse_forest-std_mse_forest)\n",
    "print(mse_svr-std_mse_svr)\n",
    "print(mse_ols-std_mse_ols)\n",
    "print(mse_ridge-std_mse_ridge)\n",
    "print(mse_lasso-std_mse_lasso)\n",
    "print(mse_enet-std_mse_enet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
